\section{Results}


\begin{itemize}[noitemsep]
	\item show demos: engineer categories (e.g. simple stock, options, news headline, ...) 
	\item own models: where do they fail?
	\item SHAP values / most important words
\end{itemize}

% =======================================================================
\subsection{Dataset Statistics}
Before benchmarking any models on the collected dataset and Fin-SoMe, we assess how similar the datasets are using descriptive statistics. Figure \ref{figure-word-counts} shows the distribution of post lengths measured as the number of words per post. It indicates that tweets tend to be shorter than StockTwits messages on average. However, the distribution of their lengths is right-skewed suggesting that Twitter is occasionally used to share longer posts while StockTwits is not.


\begin{figure}[!ht]
	\includegraphics[width=\textwidth]{assets/images/word_count_distribution.pdf}
	\caption{Number of words per post for tweets (pyFin) and StockTwits posts (Fin-SoMe)}
	\label{figure-word-counts}
\end{figure}

The post length is not the only differentiating characteristic: The distribution of class labels also significantly differs between the two datasets. Within Fin-SoMe, more than 70\% of posts are bullish while only 10\% are bearish. While the data from Twitter also contains more positive than negative tweets, the class ratio is more balanced. However, this does not necessarily imply that posts on StockTwits are generally more positive. The Fin-SoMe dataset either uses slightly different label definitions or contains labeling errors. For example, the post \emph{``\$NXT.X December 28th is the key date. Dec 25-28 this is gonna be wild!''} was labeled as ``bullish''. However, no part of the sentence indicates a clear bullish sentiment. The post author could also expect a stock price drop. Given the ambiguous nature of the text, it would fall in the ``Uncertain'' category according to the codebook used in this work.

\begin{figure}[!ht]
	\includegraphics[width=\textwidth]{assets/images/class_distributions.pdf}
	\caption{Distribution of labels within the datasets}
	\label{figure-class-distribution}
\end{figure}


\begin{itemize}[noitemsep]
	\item show diffs between datasets
	\item bad labels in FinSoMe
	\item build classifier to tell them apart?
\end{itemize}

% =======================================================================
\subsection{Model Performance Evaluation}

\begin{itemize}[noitemsep]
	\item compare model performances on own data
	\item compare model performances on finsome
	\item compare model performances on semEval
	\item financial phrase bank??
	\item compare inference times
	\item mention training times
\end{itemize}

\subsubsection{Optimal Model Configuration}
The optimal hyperparameter configuration for each model is presented in Table \ref{table-optimal-hparams}. The chosen hyperparameters suggest that for the ML models subword tokenization outperforms words-based tokenization using character n-grams of size 4. Another notable pattern is the DL models' need for high token dropouts. Both the recurrent neural net and the transformer-based neural net generalize best when 40 - 50\% of the input tokens are dropped. Additionally, the optimal embedding dimensionality is relatively low. A possible cause for this is that complex DL models require stronger regularization to not overfit smaller datasets.

\input{assets/tables/static/best_hparams.tex}




\subsubsection{Performance on the Twitter Dataset}
Figure \ref{figure-model-performance-twitter} displays each model's out-of-sample ROC AUC when applied to the dataset we collected from Twitter. The dictionary-based VADER and NTUSD-Fin perform worst scoring an AUC of 0.578 and 0.590 respectively. The domain-specific NTUSD-Fin outperforms VADER, but only by a small margin. FinBERT and TwitterRoBERTa, on the other hand, perform significantly better by both achieving an AUC score of 0.695.\newline
In comparison, all of the proposed models obtain AUC scores above 0.80. Out of these models, the recurrent neural net performs worst with an AUC of 0.803. It is outperformed by both the fine-tuned BERT model and the transformer-based neural network which score an AUC of 0.814. The two best models are the logistic regression and the SVM model which achieve AUC scores of 0.817 and 0.827 respectively.


\begin{figure}[!ht]
	\includegraphics[width=\textwidth]{assets/images/model_performance_pyfin.pdf}	
	\caption{Out-of-sample model performance on the dataset collected from Twitter}
	\label{figure-model-performance-twitter}
\end{figure}



\subsubsection{Performance on the StockTwits Dataset}
\subsubsection{Inference Times}


% =======================================================================
\subsection{Model Diagnostics}

% -----------------------------------------------------------------------
% =======================================================================
\subsection{pyFin-Sentiment}
reference the python library.