\section{Results}

% =======================================================================
\subsection{Dataset Statistics}
\label{section-dataset-diffs}
Before benchmarking any models on the collected dataset (which we will henceforth refer to as ''pyFin''), Fin-SoMe, and SemEval-2017 Task 5 (``SemEval'') \cite{cortis2017semeval}, we assess how similar the datasets are using descriptive statistics. Figure \ref{figure-word-counts} shows the distribution of post lengths measured as the number of words per post. It indicates that tweets tend to be shorter than StockTwits messages on average. However, the distribution of their lengths is right-skewed suggesting that Twitter is occasionally used to share longer posts while StockTwits is not. SemEval contains 38\% tweets and 62\% StockTwits posts and therefore exhibits a symmetric distribution of post length with a lower mean than Fin-SoMe.


\begin{figure}[!ht]
	\includegraphics[width=\textwidth]{assets/images/word_count_distribution.pdf}
	\caption{Number of words per post for tweets (pyFin), StockTwits posts (Fin-SoMe), and the SemEval dataset}
	\label{figure-word-counts}
\end{figure}

The post length is not the only differentiating characteristic: The distribution of class labels also significantly differs between the datasets (Figure \ref{figure-class-distribution}). Within Fin-SoMe, more than 70\% of posts are bullish while only 10\% are bearish. While the data from Twitter also contains more positive than negative tweets, the class ratio is more balanced. However, this does not necessarily imply that posts on StockTwits are generally more positive. The Fin-SoMe dataset either uses slightly different label definitions or contains labeling errors. For example, the post \emph{``\$NXT.X December 28th is the key date. Dec 25-28 this is gonna be wild!''} was labeled as ``bullish''. However, no part of the sentence indicates a clear bullish sentiment. The post author could also expect a stock price drop. Given the ambiguous nature of the text, it would be labeled ``uncertain'' according to the codebook used in this work. SemEval exhibits yet another class distribution where very few documents belong to the neutral class but the positive posts still outnumber the negative ones. However, as the dataset contains posts from two sources and the sampling methodology used for compiling the dataset is not available in sufficient detail, it is not representative of either one of the platforms.

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.85\textwidth]{assets/images/class_distributions.pdf}
	\caption{Distribution of labels within the datasets}
	\label{figure-class-distribution}
\end{figure}

Finally, the vocabulary used in the posts on Twitter and StockTwits is not identical. The relative frequency of words like ``Elon'', ``chatroom'', ``inflation'', and ``factory'' is between 20 and 50 times higher in the Twitter dataset than it is in the posts from StockTwits. On the other hand, the words ``blackberry'', ``silver'', ``vix'', and ``senate'' are 12 to 25 times more prevalent in StockTwits posts than in tweets. The divergence between the two sets of vocabulary can be explained by platform-specific and temporal differences. Users on Twitter are more prone to interacting with one another, for example by commenting. Additionally, the Fin-SoMe dataset was collected in or before 2020 putting more than two years between the collection of posts in pyFin and Fin-SoMe. Therefore, the two datasets have different data distributions despite covering similar domains. These differences can affect a model's predictive performance, which will be assessed in more detail in Section \ref{section-modelperf-on-finsome}. The same concept applies to any model's ability to generalize to future, unseen data. If the nature of online discussions changes significantly, older models might require re-training on current data to maintain their performance characteristics.

% =======================================================================
\subsection{Model Performance Evaluation}

\subsubsection{Optimal Model Configurations}
All subsequently reported results will be based on the optimal model configuration that is found during hyperparameter optimization.
The optimal hyperparameter configuration for each model is presented in Table \ref{table-optimal-hparams}. 

\input{assets/tables/static/best_hparams.tex}

The chosen hyperparameters suggest that for the ML models subword tokenization outperforms words-based tokenization using character n-grams of size 4. Another notable pattern is the DL models' need for high token dropouts. Both the recurrent neural net and the transformer-based neural net generalize best when 40 - 50\% of the input tokens are dropped. Additionally, the optimal embedding dimensionality is relatively low. A possible cause for this is that complex DL models require stronger regularization to not overfit smaller datasets.




\subsubsection{Performance on Twitter Posts from pyFin}
Figure \ref{figure-model-performance-twitter} displays each model's out-of-sample ROC AUC when applied to the dataset we collected from Twitter. The dictionary-based VADER and NTUSD-Fin perform worst scoring an AUC of 0.578 and 0.590 respectively. The domain-specific NTUSD-Fin outperforms VADER, but only by a small margin. FinBERT and TwitterRoBERTa, on the other hand, perform significantly better by both achieving an AUC score of 0.695.\newline
In comparison, all of the proposed models obtain AUC scores above 0.80. Out of these models, the recurrent neural net performs worst with an AUC of 0.803. It is outperformed by both the fine-tuned BERT model and the transformer-based neural network which score an AUC of 0.814. The two best models are the logistic regression and the SVM model which achieve AUC scores of 0.817 and 0.827 respectively.


\begin{figure}[!ht]
	\includegraphics[width=\textwidth]{assets/images/model_performance_pyfin.pdf}	
	\caption{Out-of-sample model performance on the pyFin dataset collected from Twitter}
	\label{figure-model-performance-twitter}
\end{figure}



\subsubsection{Performance on External Out-of-Sample Data}
\label{section-modelperf-on-finsome}
To assess each model's generalization ability and to answer RQ4, we apply all models to the StockTwits posts compiled in the Fin-SoMe and the SemEval dataset, which contains posts from both platforms. The corresponding model performances are displayed in Figure \ref{figure-model-performance-stocktwits}. The existing models' performance hardly changes compared to the ROC AUC on the tweet dataset.\newline
On Fin-SoMe, VADER now scores a slightly higher AUC of 0.590 while NTUSD-Fin achieves a score of 0.599. TwitterRoBERTa and FinBERT achieve AUC values of 0.705 and 0.710 respectively.\newline
On SemEval, VADER performs worst at an AUC of 0.564 while NTUSD-Fin's performance is better compared to the Fin-SoMe data at an AUC of 0.626. Similar to VADER, FinBERT's performance also degraded significantly as the model only scores an AUC of 0.658. While TwitterRoBERTa seems to outperform the other models, this is a spurious signal. The SemEval dataset was included as part of the training data by \shortciteA{barbieri2020tweeteval}, who designed TwitterRoBERTa. Therefore, the performance of TwitterRoBERTa on SemEval is not a true out-of-sample estimate and cannot be compared to the other metrics.

While the existing models' performance on Fin-SoMe and SemEval is similar to their performance on the pyFin dataset, all of the proposed models perform significantly worse on these two datasets from the literature. The fine-tuned BERT model only scores an AUC of 0.702 and 0.625 on Fin-SoMe and SemEval, respectively. The transformer-based and recurrent neural networks reach AUC scores of 0.707 and 0.709 on Fin-SoMe, whereas the transformer outperforms the recurrent network on SemEval with an AUC of 0.690 over 0.661. The two ML models still perform best by obtaining an AUC of 0.727 on Fin-SoMe and 0.724 (logistic regression) and 0.737 (SVM) on SemEval. Despite the relative performance decrease when applied to data from a different source, the proposed ML models still manage to outperform the existing BERT-based models. The performance decrease is likely due to the differences between the datasets as detailed in Section \ref{section-dataset-diffs}. The existing models are not affected by it because both datasets constitute out-of-distribution data for them.

\begin{figure}[!ht]
	\includegraphics[width=\textwidth]{assets/images/model_performance_finsome.pdf}	
	\caption{Model performance on the Fin-SoMe and SemEval datasets}
	\label{figure-model-performance-stocktwits}
\end{figure}



\subsubsection{Inference Times}
In times where state-of-the-art language models like GPT-3 require millions of dollars in compute budget for training \shortcite{gpt3-cost}, it is important to assess the resourcefulness of machine learning models. Therefore, we quantify each model's inference speed. Slower models require more compute time which, in a business setting, requires a larger compute budget. Figure \ref{figure-model-inference-times} presents the inference time per sample for every model. Note that the y-axis is log-scaled due to the multiple orders of magnitude that lie between the fastest and the slowest model's speed. All experiments were conducted on a system with an AMD Ryzen 5 3600 CPU and 64GB of RAM.

\begin{figure}[!ht]
	\includegraphics[width=\textwidth]{assets/images/model_inference_time.pdf}	
	\caption{Inference time per sample (ms, log-scaled)}
	\label{figure-model-inference-times}
\end{figure}

VADER and the logistic regression model provide the fastest inference times below 0.1ms per sample. Both of the neural networks that we train from scratch are slower, but still deliver inference times of below 1ms, which is on par with NTUSD-Fin. The SVM is roughly 50 times slower than the fastest models at around 2.5ms per sample. Finally, all models that are based on LLMs are multiple orders of magnitude slower than the next quickest contender. At around 100ms per sample, they are more than 1000 times slower than the logistic regression. We have to acknowledge that all neural-network-based models would benefit from being run on a graphics processing unit (GPU). However, \shortciteA{buber2018performance} suggest that GPU utilization can speed up inference by a factor of 4 or 5, which would still leave the LLM-based models far behind all others in terms of speed.

% =======================================================================
\subsection{Model Diagnostics}
The logistic regression model strikes the best balance between training- and inference speed and predictive performance. Additionally, it is easily interpretable because it is a linear model. Therefore, we choose it as our final proposed model and focus on analyzing it in this section.

\subsubsection{Topic-Specific Model Performance}
To gain a more in-depth understanding of the strengths and weaknesses of the existing models and proposed logistic regression model, we examine their performance on an array of several artificial example tweets organized into four categories. During data labeling, these categories naturally emerged from the data. The most common topics that tweets can be categorized in are ``stock ownership'', ``options trading'', ``business acumen'', and ``neutral''. Posts subsumed under the ``stock ownership'' topic discuss investing decisions that involve buying or selling stocks directly as opposed to using more complex financial instruments. The ``options trading'' group refers to tweets discussing options trades such as buying or selling puts or calls. These posts tend to contain more complex language because stock options are a more sophisticated financial instrument than stocks. The ``business acumen'' category subsumes tweets that carry implicit sentiment that can only be detected by using business knowledge. Such tweets might discuss business events or decisions that imply a market sentiment, like exceeding earnings expectations or laying off staff. Finally, the category ``neutral'' serves as a control group because a good sentiment model should be able to detect documents without any sentiment and classify them as such. Table \ref{table-example-tweets} contains example tweets for each of the four topics and displays the correct label ${y}$ as well as each model's predicted probability for the correct class ${\mathbb{P}(\hat y = y)}$ which is emphasized if it was high enough to predict the label correctly.


\input{assets/tables/static/demo-tweets.tex}

VADER fails to detect any sentiment in the documents but classifies all neutral tweets correctly. This is caused by its dictionary-based design where the model defaults to predicting neutral in the absence of any sentiment-laden words. The sentiment-laden terms that occur in the documents (e.g. ``long'') are domain-specific, hence VADER can not pick them up. NTUSD-Fin's finance-specific dictionary makes it perform better, classifying all tweets in the ``stock ownership'' category correctly. However, it fails to detect the absence of sentiment and has a strong positivity bias which can be explained by the 4:1 ratio of bullish to bearish terms in the dictionary. FinBERT correctly identifies all neutral documents but rarely detects the correct sentiment within the other categories. TwitterRoBERTa performs exceptionally well in the ``business acumen'' category which is unsurprising as it has been trained on news headlines that these tweets closely resemble. It fails to classify the sentiment in the other two topics but accurately predicts the label for the neutral tweets. Finally, the proposed logistic regression model (labeled ``pyFin'' in Table \ref{table-example-tweets}) performs well across all categories. It struggles most with implicit sentiment and negation. For example, it misclassifies some of the ``business acumen'' tweets and does not detect that ``Sold a 58P'' constitutes positive market sentiment. As it is not able to account for interactions between words, it cannot comprehend that both ``sold'' and ``58P'' (a put option with a strike price of \$58) independently suggest negative sentiment but jointly imply a positive market sentiment. A more detailed error analysis of the proposed model is presented in Section \ref{section-model-error-analysis}.


\subsubsection{Learned Patterns}
\label{section-learned-patterns}
To determine how small domain-specific models can outperform more complex generic models, we assess how the models adapted to the task of market sentiment classification. Exemplarily, we will scrutinize the proposed logistic regression model and the transformer-based neural network. For the logistic regression, we evaluate which tokens most strongly indicate the presence of each sentiment class. For the neural network, we visualize the word embeddings that capture semantic information regarding the sentiment class assignment for each word.

Table \ref{table-top-tokens-per-class} displays the tokens with the largest associated coefficients indicating these are the most informative for predicting in favor of a specific class. Most of the tokens are domain-specific and do not carry sentiment in the regular English language. However, within the domain of finance, they carry a clear sentiment. For example, the model learned that the words ``run'', ``buy'', or ``bull'' are indicative of positive sentiment. The same holds for the bearish class for which the tokens ``dump'', ``short'', and ``red'' are indicative. The predictive tokens for the neutral class are not as coherent but do contain several indicators for spam posts, for example ``chat'' which is often used for advertising paid chatrooms. The model has also learned that emojis can be predictive of sentiment, where the rocket emoji is associated with positive sentiment and the grey shade emoji is predictive of the neutral class. Additionally, the table demonstrates the advantages of subword tokenization. A Twitter user could use the words ``buuullish'' or ``bulllish'' to emphasize a positive market sentiment. While word-based tokenization fails to detect that these are different versions of the word ``bullish'', the subword tokenization recognizes that they refer to similar concepts due to the same prefix (``bull''), middle (``ulli''), and ending (``lish'').

\input{assets/tables/static/tokens-per-class.tex}

As opposed to the logistic regression, the neural networks we trained do not learn a single parameter per token. Instead, they learn word embeddings. For the transformer-based neural network, these word embeddings are 72-dimensional (see Table \ref{table-optimal-hparams}). Semantically, tokens that lie close to each other in the 72-dimensional embedding space are similar, i.e. associated with the same market sentiment label. To visualize the embeddings, the 72-dimensional vectors are projected into a two-dimensional space which can be visualized using a scatterplot. For calculating this projection, we use \emph{Uniform Manifold Approximation and Projection} \shortcite{mcinnes2018umap}, also known as UMAP. It retains the local neighborhood structure in the data such that vectors that lie close together in the high-dimensional space also appear close together in the low-dimensional space which facilitates the identification of clusters. Figure \ref{figure-tsne-word-embeddings} shows this low-dimensional representation of selected word embeddings using the embeddings learned by the domain-specific transformer model and pre-trained GloVe embeddings (50-dimensional). The unitless axes represent the two dimensions of the low-dimensional vector space. The words are colored by their associated sentiment, where green indicates positive, red indicates negative, and blue represents no sentiment.

\begin{figure}[!ht]
	\includegraphics[width=\textwidth]{assets/images/word_embeddings.pdf}
	\caption{Two-dimensional UMAP representation of selected word embeddings}
	\label{figure-tsne-word-embeddings}
\end{figure}

The figure shows that the custom word embeddings manage to capture the semantics within and between the market sentiment classes. Words indicating positive, negative, or no specific market sentiment lie close to each other in the embedding space, and the different classes form distinct clusters. The generic GloVe embeddings, on the other hand, tend to cluster together words that are similar in the generic English language. ``Red'' and ``green'' are similar as they are both colors, as are ``up'' and ``down'' (directions) and ``bullish'' and ``bearish'' (stock market jargon). Additionally, generic neutral words are scattered across the plot without any clear distinction between sentiment clusters. This suggests that generic embeddings should not be used for representing text for domain-specific tasks. For example, the cosine similarity between the embeddings of the words ``up'' and ``down'' is 0.952 making them look almost identical when used as input to a neural network. In the embedding space learned by the transformer model, their cosine similarity is $0.09$ which enables the model to clearly distinguish between the two terms.


\subsubsection{Model Error Analysis}

\label{section-model-error-analysis}
Gaining more confidence in the proposed model's predictions requires an understanding of when it performs poorly. To scrutinize this question, Table \ref{table-confusion-matrix} shows the out-of-sample confusion matrix of the model's predictions, normalized per ground-truth class. The majority of predictions for each class are correct. If the model misclassifies a data point, the confusion is most likely to occur with adjacent classes, for example, confusing positive with neutral or negative with neutral. For the negative class, however, there is a significant amount of data that is misclassified as positive.



\input{assets/tables/static/confusion_matrix.tex}

This is confirmed by the per-class precision, recall, and F1-score. These metrics are defined as follows, where $TP$ is the number of true positives, $FP$ is the number of false positives, and $FN$ is the number of false negatives.

\begin{equation}
	\textrm{precision} = \frac{TP}{TP + FP}
\end{equation}
\begin{equation}
	\textrm{recall} = \frac{TP}{TP + FN}
\end{equation}
\begin{equation}
	\textrm{F1} = 2 \cdot \frac{\textrm{precision} \cdot \textrm{recall}}{\textrm{precision} + \textrm{recall}}
\end{equation}

Table \ref{table-classification-report} presents these per-class metrics for the out-of-sample predictions. The model is equally precise for all classes which implies that the probability of any prediction being correct is around $0.64$ across classes. The F1-score, which can be interpreted as the harmonic mean between the precision and recall for a class, is almost similar for the positive and neutral class but significantly worse for the negative class due to its lower recall. 

\input{assets/tables/static/classification_report.tex}

\subsubsection{Effect of Dataset Size}
A classifier's performance is severely influenced by the quality and quantity of training data. Given fixed data quality, larger datasets help models generalize better. However, the marginal utility of additional data approaches zero if a dataset is sufficiently large \shortcite{weiss2006maximizing}. The optimal dataset size is task-specific and cannot be determined ex-ante. For the task of market sentiment analysis, we assess how different training set sizes affect the classifier's performance. Figure \ref{figure-learning-curve} shows the plot of a learning curve on our data. It is constructed by repeatedly training a classifier on smaller subsets of the data and recording its performance. We use a logistic regression model with the optimal hyperparameter configuration, fit it, and evaluate it on test sets using 5-fold cross-validation. The curve displays the average AUC (markers) and the standard deviation across folds (shaded area).

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.75\textwidth]{assets/images/learning_curve.pdf}
	\caption{Out-of-sample model performance if trained on smaller subsets of the pyFin dataset}
	\label{figure-learning-curve}
\end{figure}

The first 1,000 data points allow the model to score an AUC of 0.727, which would already outperform all existing models. Between a training dataset size of 1,000 and 4,000, each additional 1,000 data points cause a sizable increase in model performance. After the size of 4,000, the marginal performance increase is smaller. Nonetheless, the slope of the curve at $n=8,000$ indicates that collecting additional data can still yield performance improvements as the learning curve has not yet plateaued. 

% =======================================================================
\subsection{Handling Neutral, Uncertain, and Spam Posts}
\label{section-post-hoc-classes}
The class labeled as ``neutral'' subsumes multiple kinds of posts. According to the codebook (Table \ref{table-codebook}) and the subsequent merge of the ``no sentiment'' and ``uncertain'' labels, it is assigned to spam posts, posts without any sentiment, and posts with divergent or uncertain sentiment. This section analyzes how the neutral class can be differentiated from other sentiment classes and whether a four-class modeling approach accounting for uncertain posts is superior to the common three-class approach.\newline
The simplest option to handle neutral as well as uncertain posts is building a model with four possible output classes instead of three. For this purpose, we re-fit the proposed logistic regression model using the optimal hyperparameters, but this time keep all four classes instead of merging the ``no sentiment'' and ``uncertain'' class. Table \ref{table-classification-report-four-class} shows the per-class performance of the model. Besides a slight increase in recall for the positive and negative classes, the metrics are mostly similar to the three-class model with the ``no sentiment'' class resembling the previous ``neutral'' class. However, the model has a hard time classifying posts belonging to the newly added ``uncertain'' class. The precision and recall for this class indicate that the model misses most uncertain posts and if it predicts the ``uncertain'' label, the prediction is most likely incorrect.

\input{assets/tables/static/four-class-report.tex}

Another approach to a more detailed analysis of neutral posts is designing a model that can filter out all posts belonging to the ``no sentiment'' category. This would allow us to build a downstream model that differentiates between positive, negative, and truly uncertain posts. Once again, we build such a logistic regression classifier using the optimal hyperparameters. Table \ref{table-classification-report-two-class} displays the per-class performance metrics. While the model is capable of detecting posts that contain positive, negative, or uncertain sentiment (``some sentiment''), it incorrectly classifies more than half of all posts with no sentiment (recall = 0.465). This renders it unsuitable for filtering purposes, as more than half of the non-sentiment-laden posts would be missed and misclassified by a downstream model.

\input{assets/tables/static/two-class-filter-model-report.tex}

Given that both of these approaches fail to generate viable models, the three-class modeling approach is the best option for working with the data we collected.

% =======================================================================
\subsection{Example Use-Case: Analyzing the GameStop Short Squeeze of 2021}
In this section, we demonstrate an example use case that utilizes the proposed sentiment analysis model. The example we choose is purely descriptive but shows how document-level market sentiment obtained from the proposed model can be aggregated as a time series to study a phenomenon.
As an example, we study the GameStop short squeeze in early 2021. Inspired by an investment strategy proposed on YouTube, a group of retail investors collectively organized on various social media platforms to buy stock in the video game retailer GameStop, increasing its value by 30-fold within three months. The rapid increase in GameStop's stock price forced Melvin Capital, a hedge fund with a significant short position in GameStop, to cover their position resulting in losses big enough to force them to cease operations in 2022. Its public nature and the online discussions that caused this short squeeze make it a prime example for assessing the social media sentiment regarding the GameStop stock during this time.\newline
We start by querying Twitter's API for all tweets that mention GameStop's cashtag (\texttt{\$GME}) during January or February of 2021, yielding 378,799 posts. After filtering the data as outlined in Section \ref{section-dataquality}, the dataset contains 322,604 relevant tweets. We apply the proposed sentiment model to every tweet individually to classify it as positive, negative, or neutral. To aggregate the per-document data as a time series, we calculate the percentage of positive, negative, and neutral tweets per hour and plot the 24-hour rolling average of the percentage of positive and negative tweets in the upper panel of Figure \ref{figure-gme-sentiment}. The lower panel shows the difference between positive and negative tweets in percentage points. It can be considered a measure of positivity.

\begin{figure}[!ht]
	\includegraphics[width=\textwidth]{assets/images/gme_sentiment.pdf}
	\caption{Social market sentiment for the GameStop stock during January and February 2021}
	\label{figure-gme-sentiment}
\end{figure}

The figure shows that the tweets on GameStop in the chosen period exhibit a general positivity bias. Most notable, however, are the two marked dates of maximum positivity (January 14th) and minimum positivity (February 18th). On January 14th, the GameStock stock had just doubled in value within 5 days and crossed the price of \$40 for the first time. Over the next month, the stock soared and was traded at up to \$480 before slumping back to the \$50 - \$60 price range. On February 18th, the stock value had decreased over 90\% from its high and approached \$40 again.
The time series indicate that the proposed model was able to capture sentiment that, if aggregated, correlates with important market events and can be used as an explanatory variable for future studies.


% -----------------------------------------------------------------------
% =======================================================================
\subsection{pyFin-Sentiment}
To facilitate the use of the model artifact in future work, we develop and publish an easy-to-use python library. The library is called \emph{pyFin-Sentiment} and available under an MIT license at \url{https://github.com/moritzwilksch/pyfin-sentiment}. It is also published to the python package index for installation using pip. It can be installed using the following command:

\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{pip} \PY{n}{install} \PY{n}{pyfin}\PY{o}{\PYZhy{}}\PY{n}{sentiment}
\end{Verbatim}

After installing the package, using the model for inference just requires a few lines of code. The \texttt{SentimentModel} class is imported and used to download the model. The download only needs to be executed once when using the library for the first time. Subsequent calls just use the locally cached version. For now, the ``\texttt{small}'' argument refers to the logistic regression model. It is the only available model, but designing the method interface this way facilitates future expansion of the library with other model types. Next, the user can instantiate a model object and use the familiar \emph{scikit-learn} API that exposes the \texttt{.predict()} and \texttt{.predict\_proba()} methods:



\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{pyfin\PYZus{}sentiment}\PY{n+nn}{.}\PY{n+nn}{model} \PY{k+kn}{import} \PY{n}{SentimentModel}

\PY{c+c1}{\PYZsh{} the model only needs to be downloaded once}
\PY{n}{SentimentModel}\PY{o}{.}\PY{n}{download}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{small}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{n}{model} \PY{o}{=} \PY{n}{SentimentModel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{small}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Long \PYZdl{}TSLA!!}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Selling my \PYZdl{}AAPL position}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}
\PY{c+c1}{\PYZsh{} array([\PYZsq{}1\PYZsq{}, \PYZsq{}3\PYZsq{}], dtype=object)}
\end{Verbatim}

The predictions are strings $\in \{1, 2, 3\}$ where ``1'' represents \emph{positive}, ``2'' represents \emph{neutral}, and ``3'' represents \emph{negative} sentiment.



