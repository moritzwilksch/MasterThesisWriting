\section{Introduction}

The advent of social networking sites (SNS) presents the unique opportunity to tap into an enormous stream of data that users share with the world. However, most of this data comes in the form of images, videos, or text and thus is challenging to analyze. Therefore, researchers utilize automated tools to extract information from these types of media. For images, they can apply object detection to infer what kind of objects are present in a photograph. Speech detection can transcribe spoken words in videos, and named entity recognition can be utilized to recognize entities mentioned in a text. Among these technologies, sentiment analysis has been widely used by scholars and practitioners to derive actionable insights across domains. Also known as ``opinion mining'', sentiment analysis is the practice of automatically extracting sentiments or opinions from short pieces of text. Most of the time, it is measured as a real number on a continuous scale or as a categorical label like ``positive'' or ``negative''. Sentiment obtained from posts on social networks has successfully been used to detect sentiment towards political parties \shortcite{luo2022entity} or consumer products \cite{pontiki2016semeval}. If the per-document sentiment is aggregated over time, it can be used for a plethora of downstream analyses. For example, for the domain of finance, research has shown that sentiment obtained from SNS can help forecast stock market volatility \shortcite{antweiler2004all, audrino2020impact}, trading volume \shortcite{oliveira2017impact}, and even future returns \shortcite{ren2018forecasting, wilksch2022predictive}.\newline
Despite many successful applications, a problem with most sentiment analysis models is that they were designed for working with generic texts. They exploit signaling words like ``good'' or ``bad'' for determining the sentiment of a document. While it has been shown that these models perform excellently on generic social media posts \cite{al2020evaluating}, their performance on domain-specific texts is questionable. Generic sentiment models often misclassify documents containing sentiment that is easy to identify for domain experts. They lack the domain-specific connotation of terms that are only sentiment-laden in a specific domain's context. Because there are few usable domain-specific sentiment models, generic ones are still applied blindly and their output is considered ``ground truth'' for research applications studying large quantities of data. While some studies are researching alternative models for specific domains, few of them publish their models as usable artifacts to enable other researchers to benefit from more accurate sentiment assessments.

This work analyzes and proposes a solution to this issue for the domain of financial market sentiment analysis using a Design Science Research approach. Intending to build a usable model artifact that can automatically identify an author's opinion about the future of a stock's price, we collect and manually label data to compile a gold-standard dataset of finance-related tweets. We use this data to benchmark existing sentiment models trained on either generic social media data or finance-related texts. After establishing a performance benchmark of models that are popular in the literature, we design and train multiple contending sentiment models. We publish one of the proposed models as an easy-to-use python library such that future studies can utilize it for obtaining more accurate sentiment scores of finance-related social media posts. We hope this improves future research that utilizes the public social media posts of retail investors.

The remainder of this work is structured as follows. The \emph{Theoretical Background} section introduces the concepts needed for automating sentiment analysis using statistical models. It lays out how sentiment is operationalized in the literature, explains the technologies used for automating sentiment analysis, and surveys the literature on existing approaches. Finally, it highlights the research problem we aim to solve by introducing the research questions and the research paradigm we follow to answer them. The \emph{Methodology} section gives a detailed explanation of how the data used for all experiments were collected, labeled, and preprocessed. Furthermore, it lays out the experimental setup we use to train and benchmark all models. Subsequently, we present dataset statistics, an evaluation of model performance, and detailed model diagnostics in the \emph{Results} section. We highlight the issue of handling texts with no clear sentiment, provide an example use-case of how the proposed model might be used for future research, and introduce the python library that contains the final research artifact. We will discuss the emerging findings in the \emph{Discussion} and provide a final, high-level summary of the work and its findings in the \emph{Conclusion}.