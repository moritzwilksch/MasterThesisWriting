\section{Conclusion}
The purpose of this work was to provide a tool that enables researchers and practitioners to automatically assess sentiment in short, finance-related social media posts. To achieve this goal, we identified two problems in the existing literature: It lacks published, usable model artifacts that anyone can utilize for sentiment analysis which in turn forces researchers to rely on generic models when analyzing domain-specific texts. Consequently, we employed a Design Science Research approach to develop a model artifact that solves these problems and benchmarked it against existing approaches from adjacent domains. By conducting structured experiments with various statistical models on a manually labeled dataset that was collected specifically for this work, we provide insights into the technical aspects of modeling that can be used in future studies related to sentiment analysis.\newline
Our results showed that existing models that were not trained on finance-related social media posts significantly underperform all proposed models on the dataset we collected from Twitter. Dictionary-based models perform particularly poorly indicating that researchers should prefer machine-learning-based models if the text they study contains informal language. On existing datasets sampled mostly from StockTwits, TwitterRoBERTa and FinBERT match the performance of our proposed models more closely, but still underperform slightly. On smaller datasets like the one collected for this study, simpler ML-based models outperform the trained DL models suggesting that for the studied sentiment classification task linear models can achieve performance that is on par with larger transformer-based architectures. Considering the corresponding resource requirements for training and using each model, this observation made use choose the logistic regression as the final model artifact which has been published as a python library. After studying the patterns that the ML and DL models have learned, we can hypothesize that the performance gap between generic pre-trained models and our proposed models can be partially explained by the ability to understand domain-specific vocabulary. Finally, we briefly studied the idea of reframing the modeling task from a three-class task to either a four-class approach or a multi-stage model to account for posts that do not contain any sentiment. Based on the collected dataset, both approaches failed to produce viable models, potentially because of the small dataset size. Nonetheless, future research could revisit this idea as the ability to distinguish between truly neutral texts and texts that do not contain any sentiment might be valuable for selected applications.

\begin{itemize}[noitemsep]
	\item future reccs
	\item contributions to the field
\end{itemize}