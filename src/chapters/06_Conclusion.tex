\section{Conclusion}
The purpose of this work was to provide a tool that enables researchers and practitioners to automatically assess sentiment in short, finance-related social media posts. To achieve this goal, we identified two problems in the existing literature: It lacks published, usable model artifacts that anyone can utilize for sentiment analysis which in turn forces researchers to rely on generic models when analyzing domain-specific texts. Consequently, we employed a Design Science Research approach to develop a model artifact that solves these problems and benchmarked it against existing approaches from adjacent domains. By conducting structured experiments with various statistical models on a manually labeled dataset that was collected specifically for this work, we provide insights into the technical aspects of modeling that can be used in future studies related to sentiment analysis.

Our results showed that existing models that were not trained on finance-related social media posts significantly underperform all proposed models on the dataset we collected from Twitter. Dictionary-based models perform particularly poorly indicating that researchers should prefer machine-learning-based models if the text they study contains informal language. On existing datasets sampled mostly from StockTwits, TwitterRoBERTa and FinBERT match the performance of our proposed models more closely, but still underperform slightly. On smaller datasets like the one collected for this study, simpler ML-based models outperform the trained DL models. This suggests that for the studied sentiment classification task linear models can achieve performance that is on par with larger transformer-based architectures. Considering the corresponding resource requirements for training and using each model, this observation made us choose the logistic regression as the final model artifact which has been published as a python library.
After studying the patterns that the ML and DL models have learned, we can see that the performance gap between generic pre-trained models and our proposed models can be partially explained by the ability to understand domain-specific vocabulary. This observation might be useful in future studies where researchers should ensure that the text representation they choose does not corrode information that might be relevant to their modeling task. For finance-related sentiment analysis, our results have shown that generic, pre-trained word embeddings might not be a good choice for this exact reason.
Finally, we briefly studied the idea of reframing the modeling task from a three-class task to either a four-class approach or a multi-stage model to account for posts that do not contain any sentiment. Based on the collected dataset, both approaches failed to produce viable models, potentially because of the small dataset size. Nonetheless, future research could revisit this idea as the ability to distinguish between truly neutral texts and texts that do not contain any sentiment might be valuable for selected applications.

As pointed out in Section \ref{section-discussion}, when interpreting the results we obtained, one must consider the relatively small size of the labeled dataset. Additionally,  the results of this work might not generalize to different markets or data sources than the ones we studied. Therefore, future research could reassess the performance of different models (including the one proposed in this work) on larger datasets or different data sources. Considering the fast-paced environment of online discussions, future studies could scrutinize how model performance changes over time. This would shed light on how often ML models need to be re-trained to provide accurate sentiment assessments on data produced after the period that is covered by the training dataset.

Overall, this work contributed to the field of sentiment analysis in three ways. Firstly, it empirically demonstrated the performance differences between different model classes on datasets obtained by previous research as well as a newly collected dataset. Based on these results, this work can guide future research in choosing appropriate sentiment models based on the type of texts it studies. Secondly, this study was able to develop a working model artifact that is publicly available and can be utilized for automated sentiment analysis of finance-related tweets or StockTwits posts. Finally, the technical modeling insights from this project might inform future research in the field of domain-specific sentiment analysis.\newline
