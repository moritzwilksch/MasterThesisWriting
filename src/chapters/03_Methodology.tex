\section{Methodology}
% =======================================================================
\subsection{Data Collection}
% -----------------------------------------------------------------------
\subsubsection{Data Sources}
English-speaking users who discuss finance and investing on online social media platforms in text form do so on three major platforms: Reddit, StockTwits, and Twitter. Reddit is a SNS on which users can create their own communities (``subreddits'') that focus on a specific topic. For example, users have created the subreddits ``Investing'' and ``StockMarket'' to discuss long-term investments and the subreddit ``WallStreetBets'' for posts about high-risk short-term gambles in the market. However, posts on Reddit tend to be much longer than posts on Twitter or StockTwits. Their length would require them to be analyzed on a paragraph- or sentence basis. Since research on SA is mostly focused on document-level analysis, we will not use Reddit posts for this work. The decision between StockTwits and Twitter is harder: Posts on both platforms are more similar in length and share the usage of cashtags (a ``\$'' sign followed by a ticker symbol) for identifying stocks. We decide to obtain data from Twitter rather than StockTwits for the following reasons:
\begin{enumerate}[noitemsep]
	\item The post volume on Twitter is higher than it is on StockTwits.
	\item The few data sets for SA on finance-related SM posts that exist use data from StockTwits, hence publishing a data set of tweets provides more value to the research community.
	\item By using Twitter data for our experiments we can answer RQ4 and compare performances between models and data sources.
\end{enumerate}

A disadvantage of the Twitter platform is that -- unlike StockTwits -- the majority of tweets are not related to finance and investing. As a remedy, we utilize cashtags for searching investment-related posts on Twitter. These tags are only used when referring to publicly traded companies as financial entities, as each cashtag contains the company's stock ticker symbol. This mostly prevents generic tweets about a company's brand or products from spilling into the collected data and allows us to focus the artifact design process on finance-related social media posts.

% -----------------------------------------------------------------------
\subsubsection{Sampling}
\label{section-sampling}
The first step to collecting data on Twitter is assembling a search query because the Twitter search application programming interface (API) requires users to search for specific cashtags instead of any tweet containing cashtags. To make results comparable to the previous literature we will focus on English posts only. Therefore, we use the S\&P 500 index as a starting point for selecting ticker symbols to include in the search query. From there, we impose a minimum activity filter on each stock ticker: A ticker is only considered to be actively discussed on Twitter if there are more than 100 tweets per day on average mentioning it. We impose this filter because finance-related SA is only a valuable tool when applied to larger corpora of data. It should not be used when low post volume creates the risk of mistaking the opinion of very few people as the ``public'' sentiment. By using an activity filter, we ensure that the collected tweets are sampled from active discussions which make the training data more closely resemble the data that the SA models will be applied to at inference time. To conduct the filtering, we collect data on the number of tweets per day for every S\&P 500 ticker during April of 2022. As Figure \ref{figure-tweet-activity} demonstrates, the distribution of activity is highly skewed. The top 20 tickers account for 53.7\% of all tweets about S\&P 500 companies. According to the April 2022 data, 56 tickers fulfill the minimum activity constraint and account for 70.9\% of tweet volume. Out of these 56, we manually exclude 6 tickers (\texttt{AME, OGN, TEL, AMP, KEY, STX}) because while they represent corporations listed in the S\&P 500 index, they are mostly used to reference cryptocurrencies on Twitter. This is problematic as the domain of cryptocurrencies is fundamentally different from equity markets. Financial instruments like options do not exist for cryptocurrencies and they are also not affected by any kind of fundamental information. Thus, we decide to remove them from the data set and focus on tweets referring to publicly traded companies. This leaves 50 tickers to be included in the final Twitter API query.

\begin{figure}[!ht]
	\includegraphics[width=\textwidth]{assets/images/tweet_counts.pdf}
	\caption{Number of average tweets per day in April 2022 for the top 20 tickers}
	\label{figure-tweet-activity}
\end{figure}

The final API query is displayed in Figure \ref{figure-api-query}. The search retrieves all tweets mentioning any of the 50 tickers. Additionally, five options make the query 1) only retrieve English tweets, 2) exclude retweets because they do not contain original text, 3) exclude advertisements, 4) exclude tweets with images, and 5) exclude tweets with attached videos. We choose to exclude tweets with attached images or videos because we cannot analyze the information in the attached media. While most attachments are non-informative memes anyways, some include screenshots of stock charts or trading setups. Such images contain valuable information, but extracting it is beyond the scope of this work, which focuses on NLP.

\begin{figure}[!ht]
	\begin{tcolorbox}
	\small
	\centering
\texttt{(\$TSLA OR \$TWTR OR \$AAPL OR \$NFLX OR \$FB OR \$AMZN OR \$GM OR \$AMD OR \$NVDA OR \$MSFT OR \$DIS OR \$GOOGL OR \$F OR \$GOOG OR \$PYPL OR \$CAT OR \$T OR \$CVX OR \$BAC OR \$AAL OR \$BA OR \$PFE OR \$INTC OR \$JPM OR \$OXY OR \$ES OR \$WMT OR \$UAL OR \$DAL OR \$C OR \$KO OR \$XOM OR \$COST OR \$CCL OR \$MRNA OR \$MU OR \$GS OR \$WFC OR \$QCOM OR \$JNJ OR \$MS OR \$CRM OR \$SBUX OR \$VZ OR \$ABBV OR \$V OR \$MMM OR \$WBD OR \$NCLH OR \$PG) lang:en -is:retweet -is:nullcast -has:images -has:videos}
	\end{tcolorbox}
	\caption{Twitter API query used for data collection}
	\label{figure-api-query}
\end{figure}

Using the final search query, we collect all tweets using the Twitter API v2's endpoint \texttt{/2/tweets/search/all}. We query all tweets posted after April 1, 2021 (00:00:00 UTC) and before May 1, 2022 (00:00:00 UTC). The presented query yields $3,757,384$ raw results which are saved and undergo further filtering and preprocessing as described in Section \ref{section-dataquality}. 
 By collecting a little more than one full year's worth of tweets we cover one full business cycle and prevent the collected data from being biased towards a small window of time, for example, earnings season. However, we have to acknowledge that market conditions and consequently Twitter discussions change over time. Therefore, the data sample from 2021/2022 might lose relevance in the long term.


% =======================================================================
\subsection{Data Labelling}

% -----------------------------------------------------------------------
\subsubsection{Data Quality Assessment}
\label{section-dataquality}
Due to resource constraints, it is infeasible to label all of the collected data. Hence, we will randomly sample $10,000$ data points which will be manually annotated. To allocate the labeling resources efficiently we clean the entire dataset before selecting the subsample to be labeled. This ensures that time invested in labeling is not wasted by removing large amounts of spam posts that could have been removed automatically.\newline
We start by removing all hyperlinks from tweets as they do not constitute natural language. This will be important for subsequent filtering operations which rely on word counts. Next, we remove all duplicates from the dataset. There are two types of duplicates we filter. First, the filter removes duplicates based on the tweet IDs in case the API returns duplicate results. Second, since a lot of the content on Twitter is generated by bots posting the same content multiple times, we remove all tweets that have duplicate texts which are longer than 5 words. We impose this threshold because duplicated short tweets can be legitimate messages (for example: ``bought \$TSLA''). If two tweets longer than five words are duplicated, however, they are most likely a boilerplate message posted by an automated account (for example: ``They have helped me to grow my account to almost 60K in one month, 100\% recommend joining''). Next, we filter tweets based on the number of hashtags and cashtags. A manual inspection reveals that tweets without any informative content often use many different hashtags or cashtags to appear in as many searches as possible. Therefore, we exclude all tweets containing five or more cashtags or eight or more hashtags. At this point, however, the data that is left still contains many spam tweets. Most of them are shorter tweets with relatively many hashtags or cashtags, but not enough to be removed by the previous filter. Hence, we impose another filter based on the ratio of cashtags to words, hashtags to words, and mentions of other users to words. We require each of these ratios to be lower or equal to $0.5$ such that a tweet must contain at least as many words as cashtags, hashtags, and mentions. Finally, the only form of unwanted tweets that still accounts for a significant amount of data is tweets about cryptocurrencies. As discussed in Section \ref{section-sampling}, we want to exclude this kind of tweet. Similar to \shortciteA{yao2020domain}, we define a list of keywords that are frequently used by the cryptocurrency communities on Twitter and require there be less than or equal to two of these keywords in any tweet for it to be included in the final dataset. We allow for two keywords as we want to be conservative in removing data at this stage and stock market investors might also invest in cryptocurrencies. However, most tweets with three or more of these words are irrelevant. The keywords that were generated by iterative manual inspection of the filtering results are \emph{bitcoin, etherium, btc, eth, nft, token, wallet, web3, airdrop, wagmi, solana, opensea, cryptopunks, uniswap, lunar, hodl, binance, coinbase, cryptocom,} and \emph{doge}. Table \ref{table-samplesize-datacleaning} displays how the filtering stages reduce the sample size $n$.

\input{assets/tables/static/sample_size.tex}

From the final clean data set of $2,755,824$ tweets we sample $10,000$ for manual annotation. The sampling is conducted as a simple random sample. Instead of random sampling, NLP researchers can utilize a process called ``active learning'' which promises an increase in labeling speed. For active learning, a preliminary model is trained based on few annotated data points. Subsequently, the model can be applied to a large unlabeled corpus and return the data which are hardest to classify, for example by scoring the entropy of the predicted probabilities. These are then annotated by a human labeler before retraining the model and repeating the process. Active learning promises that by labeling the data that are hardest to classify, the model has more valuable information to learn from a smaller dataset which, for a data set of fixed size, should yield a superior model. Unfortunately, the research on active learning suggests that this intuitive process only outperforms random sampling when applied to heavily unbalanced data \shortcite{miller2020active}. Because this does not apply to the data we collect, we choose to stick to random sampling.


% -----------------------------------------------------------------------
\subsubsection{Task Definition}
Before labeling a subset of the collected data we need to clearly define the labeling task. This entails defining the dependent variable and laying out clear definitions for each category such that the assignment of documents to each category is as clear as possible. The dependent variable we study in this work is market sentiment. \citeA{chen2020finsome} make an important distinction between market sentiment and general author sentiment. For example, consider the sentence ``Nice, I already made a lot of money this morning and just shorted \$AAPL, this is gonna be great!''. The general sentiment in this document is positive as the author mentions previous successful trades and a great future. However, the author's market sentiment in this sentence is negative. They opened a short position in Apple Inc. (cashtag \$AAPL) which only yields a positive return if the stock price of Apple declines. The author, therefore, expects a decline in the market value of Apple shares which we consider a negative market sentiment. We choose to model market sentiment instead of author sentiment for the following two reasons: First, market sentiment information is more valuable for most downstream analysis tasks as it represents retail investors' anticipation of future events, not just their current general sentiment which does not need to correlate with equity markets \cite{chen2020finsome}. For example, an investor expecting a stock market crash might be happy about selling all their positions in time in which case the market sentiment (negative) and the general text sentiment (positive) might oppose one another.
 Second, the research gap outlined in Section \ref{section-research-gap} is most prevalent for the domain-specific task of market sentiment analysis as generic models like VADER \cite{hutto2014vader} or Twitter RoBERTa \cite{barbieri2020tweeteval} can be used to detect general author sentiment.

For the data annotation task, we decide to classify documents into four categories: bullish, bearish, uncertain, and no sentiment. While this labeling scheme is different from the more common three-class approach it can account for the difference between tweets with uncertain sentiment and tweets with no sentiment. A \emph{no sentiment} category is needed because not every document contains sentimental expressions. Thus, a two-class approach that classifies documents as either positive or negative is too simplistic to be applied to real-world data. In addition to the neutral label, we add a fourth category (\emph{uncertain}) to our labeling task because for document-level analysis it is unclear to which category a tweet containing both positive \emph{and} negative sentiment belongs. The new category can capture texts like ``I like stock A but dislike stock B''. It is important to reiterate that this category does not mean that the labeler is uncertain about which category is correct, but rather that the post contains both positive and negative aspects.
 The disadvantage of this approach is that we need a translation scheme between the labeled data obtained in this work and the more common three-class approach used by previous literature to compare model performances on different datasets. For now, we suggest that the two categories \emph{uncertain} and \emph{no sentiment} can be merged into what other research refers to as \emph{neutral}. We will adopt this approach in the modeling phase to ensure that results obtained in this work are easily comparable to previous studies. Another advantage is that the common three-class approach maps to the market sentiments known as ``bullish'' (positive), ``bearish'' (negative), and neutral. After modeling market sentiment analysis as a three-class task, we will further assess the differences between the ``no sentiment'' and ``uncertain'' categories (see Section \ref{section-post-hoc-classes}).
 Finally, Table \ref{table-codebook} presents the detailed definitions according to which the data were labeled.
 
\input{assets/tables/static/codebook.tex}


% =======================================================================
\subsection{Data Preprocessing}
\label{section-data-preprocessing}
% -----------------------------------------------------------------------
\subsubsection{Tokenization and Representation of Text Data}
The text data needs to be pre-processed and tokenized before being fed to any statistical model. To facilitate the learning of generalizable patterns, we apply the following preprocessing steps to the data before splitting it into tokens.
\begin{enumerate}[noitemsep]
	\item Replace HTML-encoded special characters, for example ``\&lt;'' by ``<''
	\item Replace all digits with the digit ``9''
	\item Replace all cashtags with the placeholder ``ticker''
	\item Replace all user mentions with the placeholder ``@user''
	\item Convert the text to lower case
\end{enumerate}

The data returned by the Twitter API contains HTML encoded special characters. To simplify each document's text, we replace the encoded character with its actual representation. Next, we replace all digits with the digit ``9''. Because numbers mentioned in tweets are often unique, a model can memorize them to classify a document correctly. However, this process makes the model overfit the training data as the memorized numbers need not occur in future tweets and are not indicative of market sentiment. By replacing all digits, we retain the meaning of a number but reduce overfitting. For example, ``99/99/9999'' still represents a date, ``\$999'' a price, and ``+9.9\%'' a relative increase. These generalizable concepts make models less prone to overfitting and improve their performance on unseen data. The same logic applies to cashtags and usernames mentioned in a tweet. They should not influence a tweet's market sentiment, hence we replace them with placeholders to facilitate generalization. For usernames, this also guarantees Twitter users' privacy and makes the model's predictions invariant with respect to the mentioned ticker or user. Finally, we convert the text to lowercase to reduce the number of unique tokens. While this loses information about original capitalization, it drastically decreases the dimensionality of the data once it is converted to a numeric representation. We do not remove stop words or stem the words in a document as these methods have been shown to reduce predictive accuracy on the task of sentiment analysis \shortcite{renault2020sentiment}. The performance impact is caused by the design of common stop word lists. They contain words like ``up'', ``down'', ``above'', and ``below'' which, for market sentiment prediction, carry sentiment information. \shortciteA{renault2020sentiment} does find, however, that preserving emojis during preprocessing is the single most important preprocessing step. Hence, we make sure that unicode emojis, as well as emojis based on punctuation, are not altered during the data preparation.

After preprocessing the text, it still needs to be tokenized and transformed to each model's required input representation. For the machine learning models we train, this representation is a BoW matrix. The BoW matrix can be created in three different ways: Its entries can represent a binary indicator of whether a token is present in a document, the number of occurrences of a token in a document, or the ``term frequency-inverse document-frequency'' (TF-IDF) score of a token. For this work, we choose to create BoW representations based on the TF-IDF score as previous research suggests that ML models perform better on TF-IDF representations than they do on raw counts \shortcite{pimpalkar2020influence}.
The TF-IDF score of a term $t$ and document $d$ is defined as
\begin{equation}
	\textrm{tfidf}(t, d) = \textrm{tf}(t, d) \times \textrm{idf}(t)
\end{equation}
where the term frequency is defined as the raw frequency of term $t$ in document $d$
\begin{equation}
	 \textrm{tf}(t, d) = f_{t,d}
\end{equation}
and the inverse document frequency is defined as
\begin{equation}
	\textrm{idf}(t) = \log \frac{1+n}{1+\textrm{df}(t, d)} + 1
\end{equation}
where $n$ is the total number of documents and the document frequency $\textrm{df(t, d)}$ is the number of documents that contain term $t$. 

While representing data as a BoW matrix does not account for the order of the tokens, DL models can process sequential data. In theory, that makes them superior for NLP applications as the order of words in natural language conveys important information. To exploit this characteristic, the data are represented as token IDs. The mapping from tokens to IDs is created based on the corpus of text. We utilize a sub-word tokenization technique called ``byte pair encoding'' \shortcite{sennrich2015neural}. For a given vocabulary size, byte pair encoding starts by splitting a text into its characters. Next, the most frequently occurring character combinations are merged. This yields a vocabulary that contains full word tokens for frequent words like ``the'' but is still flexible because it is sub-word based for infrequent combinations. For example, the infrequently occurring name ``Melfordshire'' might be split into the tokens ``mel'', ``ford'', and ``shire''. The suffix ``shire'' is a strong indication that the word refers to a name of a town or city, a pattern that statistical models can learn. This technique combines the advantages of word- and sub-word tokenization. For our experiments, we create a vocabulary of size 3,000 on the training corpus and use it to map tokens to IDs that are fed to the DL models. Based on them, the models learn word embeddings that semantically represent the tokens (see Section \ref{section-text-repr-theory}).




% =======================================================================
\subsection{Experiment Design}

%todo: this needs a section for descriptive experiments like bias check and sample tweets demo

% -----------------------------------------------------------------------
\subsubsection{Performance Evaluation}
Training a model, tuning its hyperparameters, and assessing its out-of-sample performance requires the use of three non-overlapping sets of data. Typically, an existing dataset is split into three folds, one for training, validation, and testing. The training data cannot be used for hyperparameter selection or performance assessment as the estimates would be biased and encourage memorization. The validation set is used for selecting the optimal hyperparameters but therefore cannot be used for true out-of-sample performance assessment as choosing the optimal parameter on the validation set positively biases the performance estimate.\newline
For assessing model performance in this work, we use nested cross-validation. Considering the small dataset size of $n=10,000$, a simple three-fold split would force us to evaluate model performance on a small subset of the data (1,500 data points if 15\% of the data are chosen for testing). Instead, we split the data as demonstrated in Figure \ref{figure-cv-split}. An outer cross-validation loop splits the dataset into five folds. At each split $s_i$, one of the five folds is chosen as the test set $t_i$. The remaining four folds (80\% of the dataset) are used for training and validation which is conducted by another inner cross-validation loop. The inner cross-validation repeats the procedure and splits the remaining 80\% of the data into another five folds, one of which is used for hyperparameter selection.

\input{assets/tikz_figures/cv.tex}

The nested cross-validation not only yields more reliable performance estimations but also allows us to assess an estimate's variance. However, it requires 25 times more computational power than model fitting on a single three-fold split. Therefore, we cannot use nested cross-validation for training DL models as they already take much longer to train than the ML models. For the DL models, we will stick to holding out 25\% of the dataset as test data and use a single five-fold CV on the rest of the dataset for hyperparameter optimization.



Finally, we will compare all models based on their Area Under the Receiver Operating Characteristic Curve (ROC AUC). While we will also report the accuracy, it cannot be used for comparing models across different datasets. If the class distributions between datasets differ, accuracy scores can be misleading and do not represent the actual predictive power of a model. In this case, the ROC AUC still remains representative and hence is the better measure in this case. Moreover, it does not only quantify the number of correctly classified documents but also the models certainty in the prediction. The ROC AUC ranges from zero to one, where $\textrm{AUC}=0.5$ represents random guessing and $\textrm{AUC}=1$ implies perfect predictions.

For NTUSD-Fin, we define a custom aggregation heuristic. In their paper, \shortciteA{chen2018ntusd} suggest to merely count the number of positive and negative words and define the sentiment as the difference between these two counts. However, this disregards the numeric sentiment score that they carefully assigned each word. Therefore, we decide to sum the scores for positive words and negative words separately and then normalize them to resemble probabilities. Experiments on our data show that the between the two strategies, the latter always outperforms the former. Therefore, we utilize the custom strategy to not artificially diminish the performance of NTUSD-Fin.

% -----------------------------------------------------------------------
\subsubsection{Models and Hyperparameters}
Based on previous research on SA and common model architectures used in NLP we will train ML and DL models from scratch and fine-tune a large language model on our data. The logistic regression and SVM are the most commonly applied ML models with the advantage that the logistic regression is easily interpretable. For deep learning, we train two types of sequence models on our data: a recurrent neural network and a transformer neural network using a more novel architecture. To compare our modeling results against fine-tuned LLMs like FinBert and TwitterRoBERTa, we also fine-tune a BERT-based model on our dataset. Table \ref{table-models-to-build} provides an overview of the models we train.

\input{assets/tables/static/models_to_build.tex}

To achieve optimal performance, each model requires tuning of the most important hyperparameters. For the ML models, these parameters include the configuration of the vectorizer which tokenizes the text. Table \ref{table-searchspace} shows the search space for the parameters of the logistic regression and SVM. The parameter $p_0$ determines whether texts are split into words or characters within word boundaries. Parameter $p_1$ determines how many of the pieces resulting from that split are merged into one token. We distinguish between the word- and character-based splitting to limit the vocabulary size which can exceed 50,000 for word n-grams larger than three. For the character-based split, the search space boundaries prevent meaningless tokens like single characters from being considered vocabulary. $p_2$ imposes a minimum document frequency on tokens and thus removes tokens that occur very rarely which can help the model's ability to generalize. Finally, $p_3$ corresponds to the models' inverse $\ell_2$ regularization strength. For the SVM, two additional parameters are tuned. $p_4$ is the SVM's kernel function. For the ``poly'' kernel, $p_5$ determines its degree and therefore the complexity of the model.

\input{assets/tables/static/ml-parameter-space.tex}

The DL models have a different set of hyperparameters we tune. Both the GRU and the transformer have a set of common parameters. $p_0$ determines the token dropout. It is applied directly to the input such that, on average, the model is only presented with $1 - p_0$ of the input tokens. This random removal of tokens can prevent overfitting. $p_1$, the embedding dimensionality, determines the size of each token's embedding vector. A larger embeddings size can capture more information but slows down training. $p_2$ regulates the size of the hidden layer between the GRU/Transformer output and the output layer. $p_3$ is the value for the dropout operation before and after the hidden layer. Additionally, the GRU's hidden state dimensionality is tuned with $p_4$. For the transformer, $p_5$ is the dimensionality of the internal feed-forward layer.

\input{assets/tables/static/dl-parameter-space.tex}

For fine-tuning the BERT-based model, we use DistilBERT \shortcite{Sanh2019DistilBERTAD} to transform each document to a 768-dimensional vector representation. This vector is passed through a dropout operation, a hidden layer, another dropout operation and finally to the output layer. Therefore, the model only has two hyperparameters: the hidden dimensionality and the dropout rate. Therefore, we use the same search space as for $p_2$ and $p_3$ of the other neural network models.

Using ReLU activations after all hidden layers, we train the models using the AdamW optimizer \shortcite{loshchilov2017decoupled} with a learning rate of 0.001 and batch size of 64 for a maximum of 50 epochs or until the validation loss plateaus for at least ten epochs.
We utilize the model and vectorizer implementations from scikit-learn \shortcite{scikit-learn} for the ML models and optimize the hyperparameters using optuna \shortcite{optuna_2019}. The neural networks are implemented with pyTorch \shortcite{pytorch}. For fine-tuning the BERT-based model we use the implementation of DistilBERT \shortcite{Sanh2019DistilBERTAD} in the huggingface transformers library \shortcite{wolf2020transformers}. Figure \ref{figure-methodology-flowchart} summarizes the process.

\begin{figure}[!ht]
	\includegraphics[width=\textwidth]{assets/images/methodology_flowchart.pdf}	
	\caption{Flow chart of study methodology from data collection to model fitting}
	\label{figure-methodology-flowchart}
\end{figure}







