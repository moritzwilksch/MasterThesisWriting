\section{Methodology}
% =======================================================================
\subsection{Data Collection}
% -----------------------------------------------------------------------
\subsubsection{Data Sources}
English-speaking users who discuss finance and investing on online social media platforms in text form do so on three major platforms: Reddit, StockTwits, and Twitter. Reddit is a SNS on which users can create their own communities (``subreddits'') that focus on a specific topic. For example, users have created the subreddits ``Investing'' and ``StockMarket'' to discuss long-term investments and the subreddit ``WallStreetBets'' for posts about high-risk short-term gambles in the market. However, posts on Reddit tend to be much longer than posts on Twitter or StockTwits. Their length would require them to be analyzed on a paragraph- or sentence basis. Since research on SA is mostly focused on document-level analysis, we will not use Reddit posts for this work. The decision between StockTwits and Twitter is harder: posts on both platforms are similar in length and share the usage of cashtags (a ``\$'' sign followed by a ticker symbol) for identifying stocks. We decide to obtain data from Twitter rather than StockTwits for the following reasons:
\begin{enumerate}[noitemsep]
	\item The post volume on Twitter is higher than it is on StockTwits.
	\item The few data sets for SA on financial SM posts that exist use data from StockTwits, hence publishing a data set of tweets provides more value to the research community.
	\item By using Twitter data for our experiments we can answer RQ4 and compare performances between models and data sources.
\end{enumerate}

A disadvantage of the Twitter platform is that -- unlike StockTwits -- the majority of tweets are not related to finance and investing. As a remedy, we utilize cashtags for searching investment-related posts on Twitter. These tags are only used when referring to publicly traded companies as financial entities, as each cashtag contains the company's stock ticker symbol. This mostly prevents generic tweets about a company's brand or products from spilling into the collected data and allows us to focus the artifact design process on financial social media posts.

% -----------------------------------------------------------------------
\subsubsection{Sampling}
\label{section-sampling}
The first step to collecting data on Twitter is assembling a search query because the Twitter search application programming interface (API) requires users to search for specific cashtags instead of any tweet containing cashtags. To make results comparable to the previous literature we will focus on English posts only. Therefore, we use the S\&P500 index as a starting point for selecting ticker symbols to include in the search query. From there, we impose a minimum activity filter on each stock ticker: a ticker is only considered to be actively discussed on Twitter if there are more than 100 tweets per day on average mentioning it. We impose this filter because financial SA is only a valuable tool when applied to larger corpora of data. It should not be used when low post volume creates the risk of mistaking the opinion of very few people as the ``public'' sentiment. Therefore, by using an activity filter, we ensure that the tweets that are being collected are sampled from active discussions which make the training data more closely resemble the data that the SA models will be applied to at inference time. To conduct the filtering, we collect data on the number of tweets per day for every S\&P500 ticker during April of 2022. As Figure \ref{figure-tweet-activity} demonstrates, the distribution of activity is highly skewed. The top 20 tickers account for 53.7\% of all tweets about S\&P500 companies. According to the April 2022 data, 56 tickers fulfill the minimum activity constraint and account for 70.9\% of tweet volume. Out of these 56, we manually exclude 6 tickers (\texttt{AME, OGN, TEL, AMP, KEY, STX}) because while they represent corporations listed in the S\&P500 index, they are mostly used to reference cryptocurrencies on Twitter. This is problematic as the domain of cryptocurrencies is fundamentally different from the equity markets. Financial instruments like options do not exist for cryptocurrencies and they are also not affected by any kind of fundamental information. Thus, we decide to remove them from the data set and focus on publicly traded companies. This leaves 50 tickers to be included in the final Twitter API query.

\begin{figure}[!ht]
	\includegraphics[width=\textwidth]{assets/images/tweet_counts.pdf}
	\caption{Number of average tweets per day in April 2022 for the top 20 tickers}
	\label{figure-tweet-activity}
\end{figure}

The final API query is displayed in Figure \ref{figure-api-query}. The search retrieves all tweets mentioning any of the 50 tickers. Additionally, five options make the query 1) only retrieve English tweets, 2) exclude retweets because they do not contain original text, 3) exclude advertisements, 4) exclude tweets with images, and 5) exclude tweets with attached videos. We choose to exclude tweets with attached images or videos because we cannot analyze the information in the attached media. While most attachments are non-informative memes anyways, some include screenshots of stock charts or trading setups. Such images contain valuable information, but extracting it is beyond the scope of this work, which focuses on NLP.

\begin{figure}[!ht]
	\begin{tcolorbox}
	\small
	\centering
\texttt{(\$TSLA OR \$TWTR OR \$AAPL OR \$NFLX OR \$FB OR \$AMZN OR \$GM OR \$AMD OR \$NVDA OR \$MSFT OR \$DIS OR \$GOOGL OR \$F OR \$GOOG OR \$PYPL OR \$CAT OR \$T OR \$CVX OR \$BAC OR \$AAL OR \$BA OR \$PFE OR \$INTC OR \$JPM OR \$OXY OR \$ES OR \$WMT OR \$UAL OR \$DAL OR \$C OR \$KO OR \$XOM OR \$COST OR \$CCL OR \$MRNA OR \$MU OR \$GS OR \$WFC OR \$QCOM OR \$JNJ OR \$MS OR \$CRM OR \$SBUX OR \$VZ OR \$ABBV OR \$V OR \$MMM OR \$WBD OR \$NCLH OR \$PG) lang:en -is:retweet -is:nullcast -has:images -has:videos}
	\end{tcolorbox}
	\caption{Twitter API query used for data collection}
	\label{figure-api-query}
\end{figure}

Using the final search query, we collect all tweets using the Twitter API v2's endpoint \texttt{/2/tweets/search/all}. We query all tweets posted after April 1, 2021 (00:00:00 UTC) and before May 1, 2022 (00:00:00 UTC). The presented query yields $3,757,384$ raw results which are saved and will undergo further filtering and preprocessing as described in Section \ref{section-dataquality}. 
 By collecting a little more than one full year's worth of tweets we cover one full business cycle and prevent the collected data from being biased towards a small window of time, for example, earnings season. However, we have to acknowledge that market conditions and consequently Twitter discussions change over time. Therefore, the data sample from 2021/2022 might lose relevance in the long term.


% =======================================================================
\subsection{Data Labelling}

% -----------------------------------------------------------------------
\subsubsection{Data Quality Assessment}
\label{section-dataquality}
Due to resource constraints, it is infeasible to label all of the collected data. Hence, we will randomly sample $10,000$ data points which will be manually annotated. To allocate the labeling resources efficiently we clean the entire dataset before selecting the subsample to be labeled. This ensures that time invested in labeling is not wasted by removing large amounts of spam posts that could have been removed automatically.\newline
We start by removing all hyperlinks from tweets as they do not constitute natural language. This will be important for subsequent filtering operations which rely on word counts. Next, we remove all duplicates from the dataset. There are two types of duplicates we filter. First, the filter removes duplicates based on the tweet IDs in case the API returns duplicate results. Second, since a lot of the content on Twitter is generated by bots posting the same tweet multiple times, we remove all tweets that have duplicate texts which are longer than 5 words. We choose this threshold because duplicated short tweets can be legitimate messages (for example: ``bought \$TSLA''). If two tweets longer than five words are duplicated, however, they are most likely a boilerplate message posted by an automated account (for example: ``They have helped me to grow my account to almost 60K in one month, 100\% recommend joining''). Next, we filter tweets based on the number of hashtags and cashtags. A manual inspection reveals that tweets without real content often use many different hashtags or cashtags to appear in as many searches as possible. Therefore, we exclude all tweets containing five or more cashtags or eight or more hashtags. At this point, however, the data that is left still contains many spam tweets. Most of them are shorter tweets with relatively many hashtags or cashtags, but not enough to be removed by the previous filter. Hence, we impose another filter based on the ratio of cashtags to words, hashtags to words, and mentions of other users to words. We require each of these ratios to be lower or equal to $0.5$ such that a tweet must contain at least as many words as cashtags, hashtags, and mentions. Finally, the only form of unwanted tweets that still accounts for a significant amount of data is tweets about cryptocurrencies. As discussed in Section \ref{section-sampling}, we want to exclude this kind of tweet. Accordingly, we define a list of keywords that are frequently used by the cryptocurrency communities on Twitter and require there be less than or equal to two of these keywords in any tweet for it to be included in the final dataset. We allow for two keywords as we want to be conservative in removing data at this stage and stock market investors might also invest in cryptocurrencies. However, most tweets with three or more of these words are irrelevant. The keywords that were generated by iterative manual inspection of the filtering results are \emph{bitcoin, etherium, btc, eth, nft, token, wallet, web3, airdrop, wagmi, solana, opensea, cryptopunks, uniswap, lunar, hodl, binance, coinbase, cryptocom,} and \emph{doge}. Table \ref{table-samplesize-datacleaning} displays how the filtering stages reduce the sample size $n$.

\input{assets/tables/static/sample_size.tex}

From the final clean data set of $2,755,824$ tweets we sample $10,000$ for manual annotation. The sampling is conducted as a simple random sample. Alternatively to random sampling, NLP researchers can utilize active learning which promises a speed up in labeling speed. In active learning, a preliminary model is trained based on few annotated data points. Subsequently, the model can be applied to a large unlabeled corpus and return the data which are hardest to classify. These are then annotated by a human labeler. This process promises that by labeling the data that are hardest to classify, the model has more valuable information to learn from a smaller dataset which, for a data set of fixed size, should yield a better performing model. Unfortunately, the research on active learning suggests that this intuitive process only outperforms random sampling when applied to heavily unbalanced data \shortcite{miller2020active}. Because this does not apply to the data we collect we choose to stick to random sampling.


% -----------------------------------------------------------------------
\subsubsection{Task Definition}
Before labeling a subset of the collected data we need to clearly define the labeling task. This entails defining the dependent variable and laying out clear definitions for each category such that the assignment of documents to each category is as clear as possible. The dependent variable we study in this work is market sentiment. \citeA{chen2020finsome} make an important distinction between market sentiment and general author sentiment. For example, consider the sentence ``Nice, I already made a lot of money this morning and just shorted \$AAPL, this is gonna be great!''. The author's sentiment in this document is positive as they mention previous successful trades and a great future. However, the market sentiment in this sentence is negative. The author opened a short position in Apple Inc. (cashtag \$AAPL) which only yields a positive return if the stock price of Apple declines. The author, therefore, expects a decline in the market value of Apple shares which we consider a negative market sentiment. We choose to model market sentiment instead of author sentiment for the following two reasons. First, market sentiment information is more valuable for most downstream analysis tasks as it represents retail investors' anticipation of future events, not just their current general sentiment which does not need to correlate with equity markets \cite{chen2020finsome}. For example, an investor expecting a stock market crash might be happy about selling all their positions in time in which case the market sentiment (negative) and the general text sentiment (positive) might oppose one another.
 Second, the research gap outlined in Section \ref{section-research-gap} is most prevalent for the domain-specific task of market sentiment analysis as generic models like VADER \cite{hutto2014vader} or Twitter RoBERTa \cite{barbieri2020tweeteval} can be used to detect general author sentiment.
 \textcolor{red}{Add 3rd point: 3-class maps nicely to bullish/bearish/neutral}

For the data annotation task, we decide to classify documents into four categories: bullish, bearish, uncertain, and no sentiment. While this labeling scheme is different from the more common three-class approach it can account for the difference between tweets with uncertain sentiment and tweets with no sentiment. A \emph{no sentiment} category is needed because not every document contains sentimental expressions. Thus, a two-class approach that classifies documents as either positive or negative is too simplistic to be applied to real-world data. In addition to the neutral label, we add a fourth category (\emph{uncertain}) to our labeling task because for document-level analysis it is unclear to which category a tweet containing both positive \emph{and} negative sentiment belongs. The new category can capture texts like ``I like stock A but dislike stock B''. It is important to reiterate that this category does not mean that the \emph{labeler} is uncertain about what category is correct, but rather that the post contains both positive and negative aspects.
 The disadvantage of this approach is that we need a translation scheme between the labeled data obtained in this work and the more common three-class approach used by previous literature to compare model performances on different datasets. For now, we suggest that the two categories \emph{uncertain} and \emph{no sentiment} can be merged into what other research refers to as \emph{neutral}. \textcolor{red}{We will adopt this approach in the modeling phase to ensure that results obtained in this work are easily comparable to previous studies}.
 Finally, Table \ref{table-codebook} presents the detailed definitions according to which the data were labeled.



\input{assets/tables/static/codebook.tex}




% =======================================================================
\subsection{Data Preprocessing}
\label{section-data-preprocessing}
% -----------------------------------------------------------------------
\subsubsection{Tokenization and Representation of Text Data}
\begin{itemize}[noitemsep]
	\item tokenization
	\item word embeddings
\end{itemize}






% =======================================================================
\subsection{Experiment Design}

%todo: this needs a section for descriptive experiments like bias check and sample tweets demo

% -----------------------------------------------------------------------
\subsubsection{Performance Evaluation}
\begin{itemize}[noitemsep]
	\item cross validation
	\item metrics
\end{itemize}


\input{assets/tikz_figures/cv.tex}


% -----------------------------------------------------------------------
\subsubsection{Selection of Models}


Models to build:

\input{assets/tables/static/models_to_build.tex}



\begin{itemize}[noitemsep]
	\item model types
	\item hyperparameters
\end{itemize}

% -----------------------------------------------------------------------
\subsubsection{Hyperparameters/Search Space?}


\textbf{Finally:} Show a flowchart of everything?






