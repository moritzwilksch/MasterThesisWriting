\section{Theoretical Background}
This section introduces the relevant theoretical concepts that are required for understanding the problem of market sentiment analysis. It covers the operationalization of sentiment as well as the technologies and tools used for automating sentiment analysis. Furthermore, this section will lay out what models and datasets have previously been used to conduct sentiment analysis in different domains, focussing on the two adjacent domains of finance-related sentiment, general social media sentiment, and their intersection. Figure \ref{figure-theory-overview} shows the main concepts, their relationships, and links them to the subsections of this chapter.

\input{assets/tikz_figures/theory_overview.tex}

% =======================================================================
\subsection{Sentiment Analysis}
\label{section-sa}
% -----------------------------------------------------------------------
\subsubsection{Operationalization of Sentiment}
Before surveying the literature on automated sentiment analysis models, we need to clearly define the term ``sentiment'' and its operationalization in the scientific literature. \shortciteA{munezero2014they} point out that the terms \emph{emotion}, \emph{sentiment}, and \emph{opinion} are often used interchangeably, especially in the literature on natural language processing (NLP). However, there are important differences between these terms. Emotions are moods that are not very long-lasting and are caused ``when we perceive positive or negative significant changes in our personal situation'' \shortcite[p.~3]{ben2001subtlety}. On the other hand, a sentiment is defined as ``an acquired and relatively permanent major neuropsychic disposition'' \cite[p.~16]{cattell1940sentiment}. Opinions, on the other hand, are ``personal interpretations of information formed in the mind'' \cite[p.~4]{munezero2014they} and thus require a specific piece of information to interpret. Despite the differences between these terms, they are used mostly interchangeably in the field of NLP. In particular, sentiment analysis and opinion mining refer to the same area of research \cite{liu2012book} and are more prevalent in the literature than studies on emotion extraction \shortcite{ravi2015survey}. Potentially, this is because emotions are a more complex construct and cannot be fully conveyed through text \shortcite{munezero2014they}. In this work, we use the term ``sentiment analysis'' synonymously with the term ``opinion mining'' which implies the automated extraction of opinions from data.

Even with these definitions, it is unclear how emotions, sentiments, or opinions should be operationalized. For example, emotion extraction studies like the ones conducted by \shortciteA{li2014text} or \shortciteA{aman2007identifying} tend to frame the problem as a six-class classification task where each class corresponds to one of the six basic emotions proposed by \shortciteA{ekman1971constants}. These six emotions are happiness, sadness, anger, surprise, disgust, and fear. However, \shortciteA{aman2007identifying} point out that the inter-rater reliability for this classification task can be as low as 60\%, indicating that classifying emotions is a non-trivial task even for human annotators. To exacerbate the complexity, there are variations of this measurement scale, like the ``Profile of Mood States'' questionnaire \shortcite{mcnair1971manual} which measures a different set of six emotional states. What all of these scales have in common is that they require multi-dimensional measurements to assess emotional states. \newline
Compared to emotion extraction, the literature on sentiment analysis and opinion mining often employs simpler, even one-dimensional scales. The most prominent way of operationalizing sentiment is sentiment \emph{polarity}, which categorizes each unit of analysis as positive, negative, or, in some cases, neutral \cite{ravi2015survey}. Other approaches try to evaluate the sentiment in a piece of text as a real number between -1 and 1, a one to five-star rating (e.g. movie or product reviews), or another numeric score outside of any pre-determined interval, like the difference between the number of positive and negative words.\newline
Besides the type of employed sentiment measurement, the literature can also be categorized by the unit of analysis it is concerned with. \shortciteA{liu2012book} distinguishes the units of analysis as being either document-level, sentence-level, or entity- and aspect-based. Document-level SA operates separately on every document in a corpus, e.g. each review for a product or movie or each social media post in a collection. This level of abstraction is used most frequently as it aligns with the nature of a document as a self-contained piece of text by one author. For longer documents, assigning a single score is often not adequate. In this case, sentence-level SA enables researchers to score sentiment on a per-sentence basis and subsequently calculate aggregate scores for the paragraph or document. However, this comes at the expense of ignoring the inter-sentence context. Finally, aspect-based SA does not only generate sentiment scores but links these scores to the aspects they are referring to. This is mostly used in the analysis of product reviews, where some aspects of a product are rated positively while others are not, e.g. a computer with great battery life but poor compute performance \shortcite{pontiki2016semeval}.

In addition to these definitions which are applicable in the field of NLP, other domains have their own definitions of sentiment. For the domain of finance, \citeA{aggarwal2019defining} provides an overview of the sentiment measures that researchers have historically used. In recent literature, two of them stand out as the most common: the Chicago Board Options Exchange Volatility Index (VIX) and the Put/Call Ratio (PCR). The VIX is a single, numeric score that represents the \emph{expected} level of volatility in the next month \shortcite{cboeVIX}. Its calculation is based on options on the Standard and Poor's 500 index (S\&P 500). Its forward-looking nature makes it different from usual sentiment measurements which are based on historical data. Given that volatility is defined as the standard deviation of returns, it is not directly comparable to sentiment classes like ``positive'' or ``negative'' although lower volatility is generally associated with higher returns \shortcite{zare2013monetary}. The mathematically simpler Put/Call Ratio (PCR) is the ratio between the volume of traded put options and call options. A PCR above 1 implies that put options are being traded more than call options indicating a negative market sentiment. Apart from these easily quantifiable versions of market sentiment in finance, the field starts recognizing that retail investors' emotions, sentiments, and opinions carry valuable information too. The advent of behavioral finance describes and accounts for human biases in decision-making processes, some of which can be assessed using sentiment analysis \shortcite{hirshleifer2015behavioral}.\newline 
This work focuses on sentiment analysis and opinion mining tasks, operationalized on one-dimensional scales in the form of sentiment polarity classification. By using this measurement scale the scores will be easy to quantify and the results will be more comparable to previous literature in the field.


% -----------------------------------------------------------------------
\subsubsection{Applications of Sentiment Analysis}

Sentiment analysis has a plethora of applications within academia as well as industry. Researchers have used social sentiment to study reactions to adverse events like hurricanes \shortcite{yao2020domain} or the COVID-19 pandemic \shortcite{dubey2020twitter}. Literature regarding political opinion mining demonstrates that sentiment extracted from posts on the microblogging platform Twitter correlates with public opinion to a point where it can be used to forecast election results \shortcite{o2010tweets, tumasjan2010predicting}.
Within finance, social sentiment obtained from microblogging platforms can help forecast stock market volatility \shortcite{antweiler2004all, audrino2020impact}, trading volume \shortcite{oliveira2017impact} and even future returns \shortcite{ren2018forecasting, wilksch2022predictive}.
In industry, sentiment analysis can be utilized to replace costly surveys about consumer sentiment or mine information from product reviews. This presents businesses with the opportunity to assess their customers' needs and pain points in a faster, more efficient way and might -- in some cases -- alleviate the need for expensive focus groups. For example, this can be exploited by the film industry which can use social sentiment to forecast box office revenue and movie sales \shortcite{du2014box, rui2013whose}, or manufacturers who assess what customers like and dislike about a product \shortcite{pontiki2016semeval}.
In finance, service providers like brokerages are picking up this trend and start offering social sentiment scores as additional investment research to their customers \cite{ibkr-sentiment}. The common denominator between all these use cases is that they profit from more accurate SA models. Albeit automated SA models never perform with flawless accuracy, the closer they are to what humans consider to be ground truth, the better their downstream applications perform: Descriptive statistics paint a more faithful picture of reality, forecasting models based on SA yield more accurate predictions, and for researchers, the quality of the obtained data improves.
% =======================================================================
\subsection{Machine Learning for Natural Language Processing}
% -----------------------------------------------------------------------
\subsubsection{Representation of Text Data}
\label{section-text-repr-theory}
Machine learning (ML) as well as deep learning (DL) use mathematical models that are optimized to perform a certain task as precisely as possible. The nature of these models as well as their optimization requires that all input, intermediate, and output data are represented in numeric form. No matter whether the training task requires image, text, or audio data, if it can not be represented numerically, it cannot be used for ML or DL. Accordingly, researchers in NLP have come up with ways to represent text numerically. The simplest one is known as the ``Bag-of-Words'' (BoW) method. Figure \ref{figure-bow} demonstrates an example of how BoW operates.

\input{assets/tikz_figures/bow.tex}

 First, each document's text is split into words. Then, after compiling a set of all unique words from all documents (the vocabulary), a matrix $M \in \mathbb{R}^{d \times w}$ can be constructed where $d$ is the number of documents and $w$ is the number of unique words (vocabulary size). Now, each entry $m_{i,j}$ can represent either a binary indicator of whether document $i$ contains word $j$ or the number of occurrences of term $j$ in document $i$. For the example in Figure \ref{figure-bow}, the text \emph{``sell this stock''} can be represented as the vector $(0,0,1,1,1)$. While BoW representations are fast and easy to construct, they do not retain the \emph{order} of the words in the text and hence lose important information. \newline
 A more sophisticated representation of text are word embeddings, which are the most commonly used type of text representation in DL. Word embeddings work like a look-up table. Every word is assigned a $d$-dimensional vector $v \in \mathbb{R}^{d}$. A document can consequently be represented as a sequence of such vectors. These vector representations are not chosen arbitrarily. Either, they can be learned during training of any task-specific DL model. In this case, words with similar embedding vectors often occur in documents that belong to the same class. The proximity in the vector space is often measured as the cosine similarity $S_C$, which for two vectors $\bm{a}$ and $\bm{b}$ is zero if they are orthogonal and one if they are collinear to one another. The cosine similarity is given as

\begin{equation}
	S_c(\bm{a}, \bm{b}) = \frac{\bm{a} \cdot \bm{b}}{\Vert \bm{a} \Vert_2 \cdot \Vert \bm{b} \Vert_2}
\end{equation} 

Alternatively, previous studies have developed multiple sets of pre-trained word embeddings that are freely available for download. The most prominent set of word embeddings is GloVe \shortcite{pennington2014glove} which has been trained on word co-occurrence within a corpus of Wikipedia and web data. Therefore, words that are close together in the 300-dimensional embedding space of GloVe frequently co-occur in natural English language. Table \ref{table-glove-demo} displays a set of words with their corresponding nearest neighbors in the embedding space. It confirms that after training on extensive corpora, GloVe embeddings manage to capture a considerable amount of semantic information. However, it is also evident that nearest neighbors in embedding space do not necessarily share a meaning. The words \emph{buy} and \emph{sell} have very similar word vectors ($S_c = 0.86$) while being antonyms of one another. We will highlight this issue and its effect on a market sentiment analysis model in more detail in Section \ref{section-learned-patterns}.

\input{assets/tables/static/glove-embeddings.tex}

Both of these techniques, BoW and word embeddings, can not only be applied to \emph{words}, but also \emph{sub-words}. While a word-based analysis splits a piece of text into its words (often by splitting on the space symbol between them), a sub-word-based approach splits a piece of text into multiple tokens that are smaller than words, for example, all subsequent 7-character sequences within a word. This is demonstrated in Figure \ref{figure-word-subword}. The BoW representations of the same two texts are shown, once tokenized as words (split on space), once tokenized as subwords of size seven, i.e. all sequences of seven characters within each word's boundary, padded with space to accommodate shorter words. The example makes it evident that sub-word tokenization can capture more information from the words, especially if the word stem is similar but the words have different prefixes or suffixes. The resulting cosine similarity between the phrases ``researcher in academia'' and ``academic research'' are zero for word tokenization and $0.533$ for sub-word tokenization. Thus, for these related word groups, sub-word tokenization is a more expressive text representation. This comes at the cost of more memory consumption and tokens that are harder to interpret. The same tokenization technique applies to word embeddings where instead of entire words each token is assigned an embedding vector. However, most pre-trained embeddings have been trained on entire words rather than sub-word tokens.
\input{assets/tikz_figures/word_vs_subword.tex}

% -----------------------------------------------------------------------
\subsubsection{Recurrent Neural Networks}
Once words are represented in a numeric form, which is usually word embeddings for DL, they can be used as input to neural networks. A trivial way of constructing the input to a neural net from multiple word embeddings is to just concatenate them. However, this would not preserve the sequential order of words in a piece of text. Instead, three common types of neural networks can directly work with the sequential nature of text data: recurrent neural networks, convolutional neural networks, and transformer neural networks.\newline 
 Recurrent neural networks (RNN) consume the input sequence one word at a time, as displayed in Figure \ref{figure-rnn}. At each time step $t$, the RNN processes two inputs: 1) the word embedding of the word at step $t$, $\bm{w}_t$ and 2) the output of the previous step $\bm{o}_{t-1}$, which is an $h$-dimensional vector where $h$ is a hyperparameter. This $h$-dimensional intermediate representation is also called a ``hidden state''. After the last step, the last hidden state represents the entire text sequence in one vector. This vector can then be used by other layers in the neural network to perform classification. 

\input{assets/tikz_figures/rnn.tex}

There are various different types of RNN layers which determine how output $o_t$ is calculated based on inputs $\bm{w}_t$ and $\bm{o}_{t-1}$. The simplest one, a vanilla RNN, uses a single internal operation to map the two inputs to one output. Unfortunately, it does not cope well with growing sequence lengths, because the gradient that is used for updating the weights during training decays and approaches zero, which prevents the model from learning. This is known as the \emph{vanishing gradient} problem \shortcite{hochreiter1998vanishing}. Long short-term memory (LSTM) neural networks \shortcite{hochreiter1997lstm} alleviate this issue by introducing three more operations inside the layer that allow gradient information to flow through the layer unchanged. This makes LSTMs more complex but less susceptible to the vanishing gradient problem. More recently, \shortciteA{cho2014gru} proposed the Gated Recurrent Unit (GRU), a simpler version of the LSTM with comparable performance.

% -----------------------------------------------------------------------
\subsubsection{Convolutional Neural Networks}
Apart from RNNs, NLP researchers have experimented with adapting techniques from the field of computer vision. Most of the computer vision research is based on convolutional neural networks (CNNs). Invented by \shortciteA{lecun1999object}, they address one of the main challenges of working with image data, which is working on data with spatial structure. Within an image, pixel location is a crucial piece of information. Therefore, CNNs use sliding windows that are moved across an image pixel by pixel and calculate abstract features based on every iteration. This enables them to learn local spatial relationships and makes the model invariant to the location of input signals. The same idea has been applied to text data. Once a piece of text is represented as a sequence of word vectors, a one-dimensional CNN can slide a window of size $w$ over the sequence. At each step, the network can process the $w$ words inside that window, thereby retaining the local spatial relationship. Figure \ref{figure-cnn} demonstrates two steps of this process at two intermediate steps. In the example, a sliding window of size $w = 3$ moves across the word vectors. This window, also called \emph{filter}, is a matrix $\bm{F}\in \mathbb{R}^{w \times d}$ where $w$ is the window size and $d$ the embedding dimensionality. At every step $i$ an abstract feature $\bm{r}_i$ is computed as $\bm{r_i} = \bm{F} \circ \bm{W}_{i,i+w;1,d}$ where $\circ$ denotes the Hadamard product and $\bm{W}_{i,i+w;1,d}$ is the slice of the word vector matrix $\bm{W}$ that is  within the sliding window at step $i$. The resulting vector of abstract features subsequently represents the entire piece of text.

\input{assets/tikz_figures/cnn.tex}

\shortciteA{zhang2015sensitivity} show that the CNN architecture can perform well when trained for sentence classification on GloVe word vectors. \shortciteA{sohangir2018bigdata} benchmark multiple DL models on the task of classifying StockTwits sentiment and even find CNNs to outperform other sequence models like LSTMs. Between 2016 and 2018, many studies that utilize DL for SA experimented with CNNs \shortcite{dang2020sentiment}. Since 2019, they have been mostly replaced with transformers.

% -----------------------------------------------------------------------
\subsubsection{Transformer Neural Networks}
Transformers are a neural network architecture for learning on sequences that was proposed by \shortciteA{vaswani2017attention}. In NLP, transformers replace sequence models like the RNN or LSTM. Figure \ref{figure-transformer} demonstrates how they process text: A transformer takes as input word vectors ($\bm{w}_i$) and outputs vectors of the same shape ($\bm{v}_i$). It reweights the original word vectors $\bm{w}_i$ using an attention mechanism where each word vector's reweighting operation is affected by all the other word vectors in the document. This enables the model to learn in context. For example, it would be able to differentiate between the word ``\emph{bank}'' in the two word groups: ``river bank'' and ``bank account'' based on the context. \shortciteA{vaswani2017attention} detail the internal mechanisms of the transformer block in their paper. The reweighted vectors $\bm{v}_i$ can be used for sequence-to-sequence learning tasks like machine translation or they can be averaged to form a single vector representation which can be fed to subsequent densely connected layers in a neural network to perform text classification.


\input{assets/tikz_figures/transformer.tex}

The architecture as presented in Figure \ref{figure-transformer} does not have a notion of order among the words. In practice, researchers often add positional encodings to the word vectors. These positional encodings can be learned or generated using a periodic function such that words that are close together in a document have relatively similar positional encodings \cite{vaswani2017attention}. While injecting the positional information distorts the embedding space, the transformer model can now differentiate between words that are close together or far apart in the input sequence.


% =======================================================================
\subsection{Automated Sentiment Analysis}
\label{section-automated-sa}
% -----------------------------------------------------------------------
\subsubsection{Datasets}
\label{section-datasets}
Every automated SA model -- whether dictionary-based or machine-learning-based -- requires a set of data to be built upon. For dictionaries, humans analyze large quantities of data to develop sets of words or phrases that carry sentiment and compile them in a machine-usable form. Machine learning models try to automate this process but still need a labeled training dataset that is often even larger than the ones used in manual dictionary creation. However, only a limited number of standardized, annotated datasets exist for SA, as it is costly and labor-intensive to create them. Therefore, many of the SA models in the literature have been trained and evaluated on the same datasets. Table \ref{most-used-datasets} provides an overview of the most frequently used datasets for conducting SA on texts from the domain of finance or social media.

\input{assets/tables/static/datasets_overview.tex}

The largest corpus, \emph{Reuters TRC2}, released by \shortciteA{reuters-trc2}, contains 1.8 million Reuters news articles on various topics, including financial markets. However, the dataset does not come with any kind of labels and is not exclusively geared toward performing SA. Consequently, it cannot be used for constructing dictionaries or training supervised machine learning models. Nonetheless, it can be used for pre-training large language models (LLM) which require vast unlabeled corpora for unsupervised training (see Section \ref{section-ml-based-sa}). \shortciteA{malo2014good} compiled the \emph{Financial Phrasebank}, a dataset of several thousand news headlines each of which has been annotated as either positive, negative, or neutral by 16 independent annotators. For SA on social media (SM) posts, \shortciteA{rosenthal2017semeval} provide \emph{SemEval-2017 Task 4}, a large sample of generic tweets on current events, again labeled as one of three sentiment polarity classes. On the other hand, \emph{SemEval-2017 Task 5} \shortcite{cortis2017semeval} provides data sampled from the finance-related social media context. The dataset contains a subtask (``subtask 1'') which consists of 2,510 labeled messages from StockTwits and Twitter. For each message, three annotators assign each company that is mentioned a sentiment score between -1 and 1. The scores are then consolidated by a fourth expert. While it is designed for aspect-based sentiment analysis, it is sometimes used for simpler polarity classification. The \emph{Fin-SoMe} dataset published by \shortciteA{chen2020finsome} consists of 10,000 social media posts from StockTwits, a social network to discuss stock-based investments. The platform provides users with the ability to tag their posts as bullish or bearish, which approximately 17\% of them do \shortcite{li2017learning}. Nevertheless, the authors of Fin-SoMe manually label the posts contained in their dataset. By doing so, they find that the author-assigned labels cannot be trusted entirely as 3\% of bullish and 18\% of bearish posts were tagged incorrectly by the post author \shortcite{chen2020finsome}.

 It is important to acknowledge the domain a training dataset is sampled from as it significantly impacts the performance of the resulting classifier. For example, \shortciteA{al2020evaluating} show that researchers should always use models that can cope with slang, emojis, and typos when conducting SA on social media data. On the other hand, working with corporate financial filings requires a whole new approach and renders generic models ineffective \shortcite{loughranMcD2011}. Besides the vastly different vocabulary used in various domains, the complexity of a document can also require a change of the unit of analysis. Posts on the social network \emph{Reddit} tend to be much longer than tweets, which are restricted to 280 characters in the first place. Thus, framing the SA of social media posts as a three-class classification problem might be adequate for tweets (as a tweet is likely to fall in one of the positive, negative, or neutral classes), but not so much for Reddit posts. Elaborate texts that span multiple paragraphs can rarely be categorized as belonging to one of three classes. In these cases, sentence- or paragraph-based analysis is more suitable.
 




% -----------------------------------------------------------------------
\subsubsection{Dictionary-based Models}
\label{section-dict-based}
Automated SA models can be categorized into dictionary- and machine-learning-based methods. Dictionary-based SA is a common approach that is liked for its simplicity. SA dictionaries are word lists that assign each word a score. Simple scoring methods might classify single words as positive or negative, for example LIWC \shortcite{pennebaker2001linguistic}, Harvard General Inquirer \shortcite{stone1966general}, or Opinion Observer \shortcite{liu2005opinion}. Others rate them on more sophisticated numeric scales, like ANEW \shortcite{bradley1999affective}, SentiWordNet \shortcite{baccianella2010sentiwordnet}, or VADER \shortcite{hutto2014vader}. The sentiment for a document is subsequently calculated as an aggregate of all word scores. This methodology makes dictionaries explainable and computationally cheap at inference time but does not come without drawbacks. First, the dictionary needs to be compiled by humans which is time-consuming and requires decisions regarding scales and scoring which significantly impact the performance of the final model. Second, the rigid approach of gathering a list of words can fail if documents contain few or none of the words in the list. This makes it especially hard to apply lexicon-based approaches to social media content which is riddled with typos, slang words, and emojis. Models that are not designed for this type of content often classify texts as neutral, simply for the lack of any matching words. Moreover, it is questionable whether the sentiment for a document should be determined through a simple aggregation of per-word sentiment. For example, \shortciteA{chen2018ntusd} develop an elaborate dictionary for finance-related social media posts, but define the sentiment on the document level as ``the number of positive words minus the number of negative words'' \cite[p.~42]{chen2018ntusd}. Finally, the typical challenges that automated SA entails (see Section \ref{section-sa-challenges}) have to be addressed manually. For example, \citeA{hutto2014vader} designed VADER by integrating a set of heuristics for handling negation, punctuation, and capitalization as degree modifiers of sentiment. While the authors had to invest a great amount of work into crafting these heuristics, they make VADER particularly attractive to researchers working with social media content: \shortciteA{al2020evaluating} shows that the integrated heuristics make VADER outperform all of its competitors on social media content. However, more sophisticated ML models can learn such patterns without explicit manual specification.

The domain of finance and investing entails very specific jargon and words that are either not common in everyday language or do not have a sentimental connotation. Surprisingly, there are very few lexicon-based SA approaches that are applicable to finance-related texts. The only commonly used dictionary that has been explicitly made compatible with finance-related texts is the Loughran \& McDonald lexicon \cite{loughranMcD2011}. The authors find that the majority of words classified as negative in the Harvard-IV-4 dictionary \shortcite{stone1966general} do not have a negative meaning in finance. They base their analysis on 10-K reports, the financial documents publicly traded companies have to file with the United States Securities and Exchange Commission every year. Consequently, \shortciteA{loughranMcD2011} develop multiple new word lists and change the term weighting. While their dictionary is the most commonly used finance-specific tool for SA, it fails on colloquial texts. On the other hand, many researchers still apply generic dictionaries like the Harvard-IV-4 or LIWC to domain-specific text \shortcite{kearney2014textual}. This can constitute a major methodological flaw as generic models are prone to fail when applied to such corpora \shortcite{mishev2020evaluation}.


% -----------------------------------------------------------------------
\subsubsection{Machine-Learning-based Models}
\label{section-ml-based-sa}
Sentiment Analysis lends itself to being automated through the use of machine learning models instead of manual dictionary creation. Machine learning models can be trained on large corpora of labeled data, enabling researchers to leave it to mathematical optimization algorithms to assess whether a word influences sentiment positively or negatively. While the creation of such training sets still requires significant resources and manual labor, framing SA as a machine learning problem allows for direct optimization of the correct target. Most applications of SA are not concerned with assigning a single sentiment score per word. Rather, the unit of analysis is either a sentence or a short document. ML techniques can directly optimize the objective of correctly classifying as many documents or sentences as possible. This alleviates the need for heuristics on how to aggregate word-based scores on a sentence or document level. However, the researcher's decisions regarding the type of model, data, and text preprocessing still significantly affect results.\newline
For the most part, the literature on ML-based SA can be categorized as employing either machine-learning- or deep-learning-based models. Although the discipline of deep learning is a subset of machine learning \shortcite{Goodfellow-et-al-2016}, we distinguish between the two in this work for the following reason: Deep learning utilizes deep neural networks, which are an order of magnitude more complex and expensive to train and deploy than other ML models. While neural networks are more flexible, can learn more complex dependencies, and have shown outstanding performance on many tasks in NLP, simpler ML models like the logistic regression, Na\"ive Bayes, or Support Vector Machines can still yield usable results at a fraction of the cost of large DL models. We will henceforth refer to neural-network-based models as \emph{deep learning models}, and other, less complex models as \emph{machine learning models}.

The most commonly applied ML models for SA are Support Vector Machines (SVM), Na\"ive Bayes classifiers, tree-based models, and logistic regression \shortcite{ravi2015survey}. SVM have been shown to achieve an accuracy of around 75\% on the binary classification task of assigning finance-specific posts on StockTwits a ``bullish'' or ``bearish'' label \cite{renault2020sentiment}. For a similar two-class sentiment polarity classification task of tweets that are not domain-specific, they can score accuracies as high as 83\% \shortcite{mishev2020evaluation, tang2014learning}. Na\"ive Bayes, as well as tree-based models, exhibit similar performance characteristics on generic texts \shortcite{mishev2020evaluation}. 
In other areas in the field of NLP like question answering or natural text generation all prevailing models are based on deep learning. Accordingly, researchers started applying deep learning to the task of SA, which is usually framed as a text classification problem. 
Almost all deep learning models in the field of NLP are currently leveraging LLMs which are fine-tuned to specific tasks. LLMs are sizable neural networks that have been trained on immense amounts of data. By training them on tasks like predicting masked words from a surrounding sentence, such models learn intricate patterns of natural language. Therefore, they can be used for other tasks than the one originally trained on. The representations of text that LLMs like ``Bidirectional Encoder Representations from Transformers'' (BERT) learn can be used by a single layer in a neural network to be fine-tuned on a wide variety of tasks \shortcite{devlin2018bert}. For SA of finance-related news headlines, \shortciteA{araci2019finbert} constructs FinBERT, a neural network that fine-tunes BERT on several corpora of news headlines. It outperforms all other benchmark models, including an LSTM, and reaches an accuracy of 86\% on classifying the headlines as positive, negative, or neutral. \shortciteA{barbieri2020tweeteval} develop TwitterRoBERTa, a version of RoBERTa \shortcite{liu2019roberta} that they fine-tuned on generic Twitter SA. RoBERTa itself is based on BERT but improves on key training parameters which enhances performance. This helps TwitterRoBERTa to outperform the SVM- and LSTM-based benchmarks of analyzing sentiment in tweets. Succinctly, the evidence in the literature on SA indicates that DL models outperform ML models in most benchmarks. However, few of these benchmarks consider the larger latency, training time, and deployment costs of LLMs.

Finally, it is noteworthy that while there is plenty of literature on the application of ML models to the task of SA, most of the models are not published as usable artifacts. Most of the dictionary-based or deep learning models, on the other hand, are available to researchers in the form of downloadable applications or programming libraries. Because of the applied nature of this work we can only consider models that are published as artifacts that can be used to reproduce benchmarks.


\subsubsection{Challenges of Sentiment Analysis}
\label{section-sa-challenges}
Considering the ambiguous nature of sentiments and opinions, analyzing them entails multiple challenges that need to be addressed. According to \shortciteA{hussein2018survey}, the most common SA challenges are negation handling, domain dependence, spam detection, and ambiguity in the form of abbreviations or sarcasm. Negation handling presents an issue because a few words that might not be close to the sentiment-laden part of a sentence can completely invert its meaning. In combination with domain-specific vocabulary, this can even be hard to spot for human annotators. For example, in finance, ``buying calls'' indicates a bullish sentiment whereas ``buying puts'' conveys a bearish sentiment toward a given stock. However, the nature of stock options as financial instruments allows a market participant to also \emph{sell} options, in which case the sentiment is inverted, and ``selling calls'' and ``selling puts'' convey bearish and bullish sentiments respectively. Thus, neither the words ``buy'' and ``sell'', nor the words ``put'' and ``call'' can be assigned a clearly positive or negative sentiment. This demonstrates the importance of context within a document and is particularly challenging for dictionary-based models which score documents on a word-by-word basis. Moreover, domain-specificity is not only a problem when it occurs in conjunction with negation. Different vocabulary, idioms, slang, and divergent interpretations of common words between domains can significantly degrade the quality of a SA. \citeA{ravi2015survey} provide an overview of work that addresses the challenge of cross-domain SA but conclude that it is still an unsolved problem. Consequently, researchers should be careful when applying generic SA techniques to domain-specific corpora and vice versa. On datasets obtained from social media platforms the issue of spam detection needs careful consideration. Many posts on social media are advertisements or were created by automated robots that post similar content multiple times. Not only can such duplicates ruin the quality of a collected dataset, but they also dilute the content posted by real humans as spammers try to blend in as much as possible. Removing spam is viable through heuristics developed after manual inspection of a dataset, for example by using word lists \shortcite{yao2020domain}. However, researchers must scrutinize the precision of such methods to not remove too much informative human-created content and accept that they will likely not detect 100\% of all spam posts. Arguably, the hardest challenge is coping with ambiguity and sarcasm. Using text as a medium of exchange of opinions can make these stylistic devices hard to identify even for humans. The sentence ``Yeah, company X is the best investment in the world...'' requires intonation or other cues to convey whether this statement is a sarcastic note or a serious opinion. This makes SA a problem on which even humans might not unanimously agree. Consequently, the uncertainty that is present in a training dataset carries forward to any model built on this data.





% =======================================================================
\subsection{Research Problem}
\label{section-research-gap}
The literature on SA has devised plenty of theoretical contributions as well as functional model artifacts. However, almost none of these contributions apply to the domain of SA of finance-related social media posts. Many cover movie or product reviews, because labeled data is abundantly available in the form of star ratings that are attached to the textual review. Others analyze generic social media posts and address challenges like colloquial language and the use of emojis but are still not applicable to domain-specific texts \shortcite{mishev2020evaluation}. The few works that cover finance-related texts \cite{loughranMcD2011, araci2019finbert}  do not work on social media content because of its inherent challenges for automated SA. Table \ref{table-research-gap} provides an overview of the SA models that are predominantly used in the literature. The majority of them are dictionary-based unless otherwise noted ($\dagger$).

\input{assets/tables/static/research_gap_matrix.tex}

Most models, especially older ones developed before 2010, are neither finance-specific nor do they work on social media content. In recent years, the development of models that work on SM content has flourished and made major advancements, for example with the introduction of VADER \shortcite{hutto2014vader}, or, more recently, Twitter RoBERTa \shortcite{barbieri2020tweeteval}. The literature on finance-specific texts is scarce. \shortciteA{loughranMcD2011} published a word list based on 10-K corporate filings, and more recently \shortciteA{araci2019finbert} published FinBERT which has been trained on news headlines. The gap in the existing research is consequently the intersection of finance-related social media posts. While \shortciteA{sohangir2018bigdata} experiment with deep learning models for predicting sentiment labels on StockTwits, they do not publish the final model artifact. This leaves NTUSD-Fin by \shortciteA{chen2018ntusd} who also developed a model for classifying StockTwits posts. Based on the labels that StockTwits users assign to their posts, they assemble a list of words with corresponding sentiment scores which is available upon request to the authors.

In order to add to the body of existing literature, this work aims to develop and publish a sentiment model designed for working on finance-related short-form texts sourced from social networking sites. Its development will be guided by the following research questions.\newline
\textbf{\emph{RQ1:}} How can we design a functional model artifact that can extract an author's sentiment from finance-related social media posts?\newline
\textbf{\emph{RQ2:}} Does this model artifact outperform existing models from either the domain of finance-related texts or generic social media posts?\newline
\textbf{\emph{RQ3:}} Can a simple, domain-specific model outperform more generic LLMs?\newline
\textbf{\emph{RQ4:}} How does the performance of models trained on Twitter posts change when applied to StockTwits posts?

\emph{RQ1} is aimed at the development and publication of a working model artifact. This is important to further the research in the field and enable other researchers and practitioners to apply the developed model to their data. This question formulates the main goal of this work to make automated sentiment assessments available to anyone studying downstream tasks that rely on finance-related sentiment sourced from social media. \emph{RQ2} studies the performance differences one can expect from using domain-specific models over generic models on domain-specific texts. This information will empower researchers to make better decisions regarding the SA model they use for their research. \emph{RQ3} tries to assess how LLMs, which are commonly used but expensive to train and deploy, compare to smaller models that have been trained on the target domain. It has been shown that for domain-specific computer vision applications, smaller, specialized models can outperform large off-the-shelf models for a fraction of the training- and deployment costs \shortcite{harl2022light}. This raises the question of whether the same holds for SA applications. Finally, \emph{RQ4} scrutinizes the differences between the domain of Twitter posts and StockTwits posts. Studies based on posts obtained from StockTwits are more prevalent in the literature because of the naturally available labels. However, its similarity to Twitter has never been assessed. Answering RQ4 will help guide future research and might warrant the applications of models trained on StockTwits posts to other social media platforms and vice versa.


% =======================================================================
\subsection{Research Paradigm}

To answer the research questions posed in Section \ref{section-research-gap} we adapt the process of Design Science Research as proposed by \shortciteA{kuechler2012dsrprocess} and \citeA{gregor2013dsr}. It consists of a series of five steps: Being aware of the problem, suggesting possible solutions, developing artifacts that instantiate these solutions, evaluating whether the developed artifacts solve the problem, and presenting a coherent conclusion that fosters future research \cite{kuechler2012dsrprocess}. Figure \ref{figure-dsr-process} displays the process steps along with the concrete application to answering the research questions presented in this work.\newline
The problem has been identified as the lack of functional model artifacts that can be used for automated SA of finance-related social media posts. Consequently, we suggest developing such a model that can extract the opinion of an author regarding an investment from a social media post. Considering the lack of data in this domain, the development process will entail data collection, cleaning, and labeling. Subsequently, we can experiment with different ML and DL-based modeling approaches. All of these models will be evaluated and compared against existing models from the domain of finance or SM. Finally, the best models will be deployed as an installable python library, such that future research can conduct further benchmarking and improve upon the results.

\input{assets/tikz_figures/dsr_process.tex}









