\section{Theoretical Background}



% =======================================================================
\subsection{Sentiment Analysis}  % TODO: wording
% -----------------------------------------------------------------------
\subsubsection{Operationalization of Sentiment}
% todo: vvv weak line of argument
Before surveying the literature on automated sentiment analysis models, the term ``Sentiment'' and its operationalization in the scientific literature need to be clearly defined. \shortciteA{munezero2014they} note that the terms \emph{emotion}, \emph{sentiment}, and \emph{opinion} are often used interchangeably, especially in the literature on NLP. However, there are important distinctions to be made between these terms. Emotions can be seen as moods that are not very long-lasting and are caused ``when we perceive positive or negative significant changes in our personal situation'' \shortcite[p.~3]{ben2001subtlety}. On the other hand, a sentiment is defined as ``an acquired and relatively permanent major neuropsychic disposition'' \cite[p.~16]{cattell1940sentiment}. This renders sentiments different from opinions, which are ``personal interpretations of information formed in the mind'' \cite[p.~4]{munezero2014they} and thus require a specific piece of information to interpret. %todo: wording
Despite the differences in the definition of these terms, they are used in ambiguous ways in the field of NLP. In particular, sentiment analysis and opinion mining refer to the same area of research \cite{liu2012book} and are more prevalent than studies on emotion extraction \shortcite{ravi2015survey}. Potentially, this is because emotions are a more complex construct and cannot be fully conveyed through text \shortcite{munezero2014they}.

Another challenge is the operationalization of emotions, sentiments, or opinions. For example, emotion extraction studies like the ones conducted by \shortciteA{li2014text} or \shortciteA{aman2007identifying} tend to frame the problem as a six-class classification task where each class corresponds to one of the six basic emotions proposed by \shortciteA{ekman1971constants}. These six emotions are happiness, sadness, anger, surprise, disgust, and fear. However, \shortciteA{aman2007identifying} point out that the inter-rater reliability for this classification task on a corpus of text can be as low as 60\%, indicating that emotion extraction is a non-trivial task even for human annotators. There are variations of this measurement scale, like the ``Profile of Mood States'' questionnaire \shortcite{mcnair1971manual} which measures a different set of six emotional states. What all of these scales have in common is that they are multi-dimensional measurements to assess emotional states. \newline
On the other hand, the literature on sentiment analysis and opinion mining often employs simpler, even one-dimensional scales. The most prominent way of operationalizing sentiment is sentiment \emph{polarity}, which categorizes each unit of analysis as positive, negative, or, in some cases, neutral \cite{ravi2015survey}. Other approaches try to evaluate the sentiment in a piece of text as a real number between -1 and 1, a one to five-star rating (e.g. movie or product reviews), or another numeric score outside of any pre-determined interval.\newline
Besides the type of the employed sentiment measurement, the literature can also be categorized according to the unit of analysis it is concerned with. \shortciteA{liu2012book} distinguishes the units of analysis as being either document-level, sentence-level, or entity- and aspect-based. Document-level SA operates separately on every document in a corpus, e.g. each review for a product or movie or each social media post in a collection. This granularity is used most often as it aligns with the nature of a document as a self-contained piece of text by one author. For longer documents, sentence-level SA enables researchers to score sentiment on a per-sentence basis and subsequently calculate aggregate scores for the document. However, this comes at the expense of ignoring the inter-sentence context. Finally, aspect-based SA does not only generate sentiment scores but links these scores to the aspects they are referring to. This is mostly used in the analysis of product reviews, where some aspects of a product are rated positively while others are not, e.g. a computer with great battery life but low compute performance \shortcite{pontiki2016semeval}.

In addition to these definitions which are applicable in the field of NLP, other domains have their own definitions of sentiment. For the domain of finance, \citeA{aggarwal2019defining} provides an overview of all sentiment measures that have been historically used. In recent literature, two of them stick out as the most common: the Chicago Board Options Exchange Volatility Index (VIX) and the Put/Call Ratio (PCR). The calculation of the VIX is based on options on the Standard and Poor's 500 index (S\&P500) and represents the \emph{expected} level of volatility in the next month \shortcite{cboeVIX}. Its forward-looking nature makes it different from usual sentiment measurements which are based on historical data. Given that volatility is defined as the standard deviation of returns, it is not directly comparable to sentiment classes like ``positive'' or ``negative'' although lower volatility is generally associated with higher returns \shortcite{zare2013monetary}. The mathematically simpler Put/Call Ratio is the ratio between the volume of traded put options and call options. A PCR above 1 implies that put options are being traded more than call options indicating a negative market sentiment. Apart from these easily quantifiable versions of market sentiment in finance, the field starts recognizing that retail investors' emotions, sentiments, and opinions carry valuable information. The advent of behavioral finance describes and accounts for human biases in decision making processes, some of which can be assessed using sentiment analysis \shortcite{hirshleifer2015behavioral}. Therefore, this work focuses on sentiment analysis and opinion mining tasks, operationalized on uni-dimensional scales like sentiment polarity classification or valence scoring instead of emotion extraction.


% -----------------------------------------------------------------------
\subsubsection{Applications of Sentiment Analysis}

Sentiment analysis has a plethora of applications within academia as well as industry. Researchers have used social sentiment to study reactions to adverse events like hurricanes \shortcite{yao2020domain} or the COVID-19 pandemic \shortcite{dubey2020twitter}. Literature regarding political opinion mining demonstrates that sentiment extracted from posts on the microblogging platform Twitter correlates with public opinion to a point where it can be used to forecast election results \shortcite{o2010tweets, tumasjan2010predicting}.
Within finance, social sentiment obtained from microblogging platforms can help forecast stock market volatility \shortcite{antweiler2004all, audrino2020impact}, trading volume \shortcite{oliveira2017impact} and sometimes even future returns \shortcite{ren2018forecasting, wilksch2022predictive}.
In industry, sentiment analysis can be utilized to replace more costly surveys about consumer sentiment or mine information from product reviews. This presents businesses with the opportunity to assess their customers' needs and pain points in a faster, more efficient way and might -- in some cases -- alleviate the need for expensive focus groups. For example, this can be exploited by the film industry which can use social sentiment to forecast box office revenue and movie sales \shortcite{du2014box, rui2013whose}, or manufacturers who survey customers on what they like and dislike about a product \shortcite{pontiki2016semeval}.
In finance, service providers like brokerages are picking up this trend and start offering social sentiment scores as additional investment research to their customers \cite{ibkr-sentiment}. The common denominator between all these use cases is that they profit from more accurate SA models. Albeit automated SA models never perform with flawless accuracy, the closer they are to what humans consider to be ground truth, the better their downstream applications perform: descriptive statistics paint a more faithful picture of reality, forecasting models based on SA yield more accurate predictions, and for researchers, the quality of the obtained data more closely resembles the one of focus group or survey data.


% =======================================================================
\subsection{Automated Sentiment Analysis}

% -----------------------------------------------------------------------
\subsubsection{Data Sets}

Every automated SA model -- whether dictionary-based or machine-learning-based -- requires a set of data to be built upon. For dictionaries, humans analyze large quantities of data to develop sets of words or phrases that carry sentiment and compile them in a machine-usable form. Machine learning models try to automate this process but still need a training data set that is often even larger than the ones used in manual dictionary creation. However, only a limited number of standardized, annotated data sets exist, as it is costly and labor-intensive to create such a data set. Therefore, many of the SA models in the literature have been trained or evaluated on similar data sets. Table \ref{most-used-datasets} provides an overview of the most frequently used data sets for conducting SA on texts from the domain of finance or social media.

\input{assets/tables/static/datasets_overview.tex}

The largest corpus, \emph{Reuters TRC2}, released by \shortciteA{reuters-trc2}, contains 1.8 million Reuters news articles on various topics, including financial markets. However, the data set does not come with any kind of labels and is not exclusively geared towards performing SA. Consequently, it cannot be used for constructing dictionaries or training supervised machine learning models. Despite that, it can be used for pre-training large language models (LLM) which require vast unlabeled corpora for unsupervised training (see section \textcolor{red}{REF LLM sec}). \shortciteA{malo2014good} compiled the \emph{Financial Phrasebank}, a data set of several thousand news headlines where each headline has been annotated as either positive, negative, or neutral by 16 independent annotators. For SA on social media posts, \shortciteA{rosenthal2017semeval} provide \emph{SemEval-2017 Task 4}, a large sample of generic tweets on current events, again labeled as one of three sentiment polarity classes. On the other hand, \emph{SemEval-2017 Task 5} \shortcite{cortis2017semeval} provides data sampled from the financial social media context. The data set contains a subtask (``subtask 1'') which consists of 2,510 labeled messages from StockTwits and Twitter. For each message, three annotators assign each company that is mentioned a sentiment score between -1 and 1. The scores are then consolidated by a fourth expert. The \emph{Fin-SoMe} data set published by \shortciteA{chen2020finsome} consists of 10,000 social media posts from StockTwits. StockTwits is a social network to discuss stock-based investments. The platform provides users who post with the ability to tag their posts as bullish or bearish, which approximately 17\% of them do \shortcite{li2017learning}. Nevertheless, the authors of Fin-SoMe manually label the posts contained in their data set. By doing so, they find that the author-assigned labels cannot be trusted entirely as 3\% of bullish and 18\% of bearish posts were tagged incorrectly by the post author \shortcite{chen2020finsome}.

 It is important to acknowledge the domain a training data set is sampled from as it significantly impacts the performance of the resulting classifier on its target data. For example, \shortciteA{al2020evaluating} show that researchers should always use models that can cope with slang, emojis, and typos when conducting SA on social media data. On the other hand, working with corporate financial filings requires a whole new approach and renders generic models ineffective \shortcite{loughranMcD2011}. Besides the vastly different vocabulary used in different domains, the complexity of a document can also require a change of the unit of analysis. Posts on the social network \emph{Reddit} tend to be much longer than Tweets, which are restricted to 280 characters in the first place. Thus, framing the SA of social media posts as a three-class classification problem might be adequate for Tweets (as a Tweet is likely to fall in one of the positive, negative, or neutral classes), but not so much for Reddit posts. Elaborate texts that span multiple paragraphs and might be a reply to a previous conversation can rarely be categorized as belonging to one of three classes. In these cases, sentence- or paragraph-based analysis is more suitable.
 




% -----------------------------------------------------------------------
\subsubsection{Dictionary-based Models}
Automated SA models can be categorized into dictionary- and machine-learning-based methods. Dictionary-based SA is a common approach that is liked for its simplicity. SA dictionaries are word lists that assign each word a score. Simple scoring methods might classify single words as positive or negative, for example LIWC \shortcite{pennebaker2001linguistic}, Harvard General Inquirer \shortcite{stone1966general}, or Opinion Observer \shortcite{liu2005opinion}. Others rate them on more sophisticated numeric scales, like ANEW \shortcite{bradley1999affective}, SentiWordNet \shortcite{baccianella2010sentiwordnet}, or VADER \shortcite{hutto2014vader}. The sentiment for a document is subsequently calculated as an aggregate of all word scores. This methodology makes dictionaries explainable and computationally cheap at inference time but does not come without drawbacks. First, the dictionary needs to be compiled by humans which is time-consuming and requires decisions regarding scales and scoring which significantly impact the performance of the final model. Second, the rigid approach of gathering a list of words can fail if documents contain few or none of the words in the list. This makes it especially hard to apply lexicon-based approaches on social media content which is riddled with typos, slang words, and emojis. Models that are not designed for this type of content often classify texts as neutral, simply for the lack of any matching words. Moreover, it is questionable whether the sentiment for a document should be determined through a simple aggregation of per-word sentiment. For example, \shortciteA{chen2018ntusd} develop an elaborate dictionary for financial social media posts, but define the sentiment on the document level as ``the number of positive words minus the number of negative words'' \cite[p.~42]{chen2018ntusd}. Finally, the typical challenges of SA (see section \ref{section-sa-challenges}) have to be addressed manually. For example, \citeA{hutto2014vader} designed VADER by integrating a set of heuristics for handling negation, punctuation, and capitalization as degree modifiers of sentiment. These features make VADER particularly attractive to researchers working with social media content, as the integrated heuristics make it outperform all its competitors on social media content \cite{al2020evaluating}.

The domain of finance and investing entails very specific jargon and words that are either not common in everyday language or do not have a sentimental connotation. Surprisingly, there are very few lexicon-based SA approaches that are applicable to financial texts. The only commonly used dictionary that has been explicitly made compatible with financial texts is the Loughran \& McDonald lexicon \cite{loughranMcD2011}. The authors find that the majority of words classified as negative in the Harvard-IV-4 dictionary \shortcite{stone1966general} do not have a negative meaning in finance. They base their analysis on 10-K reports, the financial documents publicly traded companies have to file with the United States Securities and Exchange Commission every year. Consequently, \shortciteA{loughranMcD2011} develop multiple new word lists and change the term weighting. While their dictionary is the most commonly used finance-specific tool for SA, many researchers still apply generic dictionaries like the Harvard-IV-4 or LIWC to domain-specific text \shortcite{kearney2014textual}. For the most part, this constitutes a major methodological flaw as generic models are prone to fail when applied to such corpora \shortcite{mishev2020evaluation}.


% -----------------------------------------------------------------------
\subsubsection{Machine-Learning-based Models}

Sentiment Analysis lends itself to being automated through the use of machine learning (ML) models instead of manual dictionary creation. Machine learning models can be trained on large corpora of labeled data, enabling researchers to leave it to mathematical optimization algorithms to decide whether a word influences sentiment positively or negatively. While the creation of such training sets still requires significant resources and manual labor, framing SA as a machine learning problem allows for direct optimization of the correct target \textcolor{red}{wording!}. Most applications of SA are not concerned with assigning a single sentiment score per word. Rather, the unit of analysis is either a sentence or a short document. ML techniques can directly optimize the objective of classifying as many documents or sentences as correctly as possible. This alleviates the need for heuristics on how to aggregate word-based scores on a sentence or document level.\newline
For the most part, the literature on ML-based SA can be categorized as employing either machine-learning- or deep-learning-based models. Although the discipline of deep learning (DL) is a subset of machine learning \shortcite{Goodfellow-et-al-2016}, we distinguish between the two in this work for the following reason: Deep learning utilizes deep neural networks, which are an order of magnitude more complex and expensive to train and deploy than other ML models. While neural networks are more flexible, can learn more complex dependencies, and have shown outstanding performance on many tasks in NLP, simpler ML models like the logistic regression, Na\"ive Bayes, or Support Vector Machines can still yield usable results at a fraction of the cost of large DL models. We will henceforth refer to neural-network-based models as \emph{deep learning models}, and other, less complex models as \emph{machine learning models}.

The most commonly applied ML models for SA are Support Vector Machines (SVM), Na\"ive Bayes classifiers, tree-based models, and logistic regression \shortcite{ravi2015survey}. SVM have been shown to achieve an accuracy of around 75\% on the binary classification task of assigning finance-specific posts on StockTwits a ``bullish'' or ``bearish'' label \cite{renault2020sentiment}. For the same two-class sentiment polarity classification task of tweets that are not domain-specific, they can score accuracies as high as 83\% \cite{mishev2020evaluation, tang2014learning}. Na\"ive Bayes, as well as tree-based models, exhibit similar performance characteristics on generic texts \shortcite{mishev2020evaluation}. 

\textcolor{red}{[Block: BoW representation]}All of these ML models work on the Bag-of-Words (BoW) representation of text data. BoW is a technique to represent texts as numeric values, which is necessary for inputing them to any ML model. Figure \ref{figure-bow} demonstrates an example of how BoW operates.

\input{assets/tikz_figures/bow.tex}

 First, each documents' text is split into words. Then, after compiling a set of all unique words from all documents (the vocabulary), a matrix $M \in \mathbb{R}^{d \times w}$ can be constructed where $d$ is the number of documents and $v$ is the number of unique words (vocabulary size). Now, each entry $m_{i,j}$ can represent either a binary indicator of whether document $i$ contains word $j$ or the number of occurrences of term $j$ in document $i$. For the example in Figure \ref{figure-bow}, the text \emph{``sell this stock''} can be represented as the vector $(0,0,1,1,1)$. While BoW representations are fast and easy to construct, they do not retain the \emph{order} of the words in the text and hence lose important information.
 %todo: compare my accuracy later!

In other areas in the field of NLP like question answering or natural text generation all prevailing models are based on deep learning. Accordingly, they have also been applied to the task of SA, which is usually framed as a text classification problem. 

\textcolor{red}{[Block: RNN \& representation of text data]} Unlike ML models, DL models in NLP usually do not operate on text in its BoW representation. A more common representation for text data in deep learning are word embeddings. Word embeddings work like a look-up table. Every word is assigned a $d$-dimensional vector $v \in \mathbb{R}^{d}$. A document can consequently be represented as a sequence of such vectors. These vector representations are not chosen arbitrarily. Either, they can be learned during training of the final classification model. In this case, words that lie close together in the embedding vector space often occur in documents that belong to the same class. The proximity in the vector space is defined as the cosine similarity $S_C$, which for two vectors $\textbf{a}$ and $\textbf{b}$ is given as

\begin{equation}
	S_c(\textbf{a}, \textbf{b}) = \frac{\textbf{a} \cdot \textbf{b}}{\Vert \textbf{a} \Vert \cdot \Vert \textbf{b} \Vert}
\end{equation} 

Alternatively, multiple pre-trained word embeddings have been developed in previous studies. The most prominent set of word embeddings is GloVe \shortcite{pennington2014glove} which has been trained on word co-occurence within a corpus of Wikipedia and web data. Therefore, words that are close together in the 300-dimensional embedding space frequently co-occur in natural English language. Table \ref{table-glove-demo} displays a set of word with their corresponding nearest neighbors in the embedding space. It confirms that after training on extensive corpora, GloVe embeddings manage to capture a considerable amount of semantic information. However, it is also evident that nearest neighbors in embedding space do not necessarily share a meaning. The words \emph{buy} and \emph{sell} have very similar word vectors ($S_c = 0.86$) while being antonyms of one another.

\input{assets/tables/static/glove-embeddings.tex}


\begin{table}[!ht]
\centering

\begin{tabular}{clllll}
\toprule
\textbf{Word} & \multicolumn{5}{c}{\textbf{Closest neighbors in embedding space}}\\
\midrule

\textbf{stock} & shares & stocks & market & exchange & trading\\
\textbf{buy} & sell & purchase & buying & bought & acquire\\
\textbf{money} & funds & cash & fund & donations & pay\\
\textbf{company} & companies & subsidiary & firm & co. & venture\\

\bottomrule
\end{tabular}
\caption{A set of four words and their closest neighbors in embedding space}
\label{table-glove-demo}
\end{table}




%\input{assets/tikz_figures/word_embeddings.tex}


Once words are represented in the form of numeric word embeddings, they can be used as input to neural networks. A trivial way of constructing the input to a neural net from multiple word embeddings is to just concatenate them. However, this would not preserve the sequential order of words in a piece of text. Instead, there are two common types of neural networks that can directly work with the sequential nature of text data: recurrent neural networks and transformer neural networks. Recurrent neural networks (RNN) consume the input sequence one word at a time, as displayed in Figure \ref{figure-rnn}. At each time step $t$, the RNN processes two inputs: 1) the word embedding of the word at step $t$, $w_t$ and 2) the output of the previous step $o_{t-1}$, which is an $h$-dimensional vector where $h$ is a hyperparameter. This $h$-dimensional intermediate representation is also called a ``hidden state''. After the last step, the last hidden state represents the entire text sequence in one vector. This vector can then be used by other layers in the neural network to perform classification. 

\input{assets/tikz_figures/rnn.tex}

There are various different types of RNN layers which determine how output $o_t$ is calculated based on inputs $w_t$ and $o_{t-1}$. The simplest one, a vanilla RNN, uses a single internal operation to map the two inputs to one output. Unfortunately, it does not cope well with growing sequence lengths, because the gradient that is used for updating the weights decays and approaches zero, which prevents the model from learning. This is known as the \emph{vanishing gradient} problem \shortcite{hochreiter1998vanishing}. Long short-term memory (LSTM) neural networks \shortcite{hochreiter1997lstm} alleviate this issue by introducing three more operations inside the layer that allow gradient information to flow through the layer unchanged. This makes LSTMs more complex but less prone to the vanishing gradient problem. More recently, \shortciteA{cho2014gru} proposed the Gated Recurrent Unit (GRU), a simpler version of the LSTM with comparable performance.


\begin{itemize}[noitemsep]
	\item See Dang20!
	\item Note: there aren't really any non-DL ML models out there. Tho: there is a lot of research on ML models, buuut there are no published artifacts. These are mostly dictionaries/DL!
	\item Recurrent models
	\item CNNs!
	\item Transformer models
	\item LLMs
	\item pre-processing / text representation (see section \ref{section-data-preprocessing})
	\item $\rightarrow$ just mention technicalities here briefly, cover tech deets in methodology??
\end{itemize}

It is noteworthy that while there is plenty of literature on the application of ML models to the task of SA, most of the models are developed using generic data sets like reviews or social media posts and are not published as usable artifacts. This is opposed to many dictionary-based models as well as deep learning models that are available to researchers in the form of downloadable models or programming libraries. Because of the applied nature of this work we can only consider models that are published as artifacts that can be used to reproduce benchmarks.


\subsubsection{Challenges of Sentiment Analysis}
\label{section-sa-challenges}
Considering the ambiguous nature of sentiments and opinions, analyzing them entails multiple challenges that need to be dealt with. According to \shortciteA{hussein2018survey}, the most common SA challenges are negation handling, domain dependence, spam detection, and ambiguity in the form of abbreviations or sarcasm. Negation handling presents an issue because very few words that might not be in direct proximity to the sentiment-laden part of a sentence can completely invert its meaning. In combination with domain-specific vocabulary, this can even be hard to spot for human annotators. For example, in finance, ``buying calls'' indicates a bullish sentiment whereas ``buying puts'' conveys a bearish sentiment toward a given stock. However, the nature of stock options as financial instruments allows a market participant to also \emph{sell} options, in which case the sentiment is reverted, and ``selling calls'' and ``selling puts'' convey bearish and bullish sentiments respectively. Thus, neither the words ``buy'' and ``sell'', nor the words ``put'' and ``call'' can be assigned a clearly positive or negative sentiment. This demonstrates the importance of context within a document and is particularly challenging for dictionary-based models which score documents on a word-by-word basis. Moreover, domain-specificity is not only a problem when it occurs in conjunction with negation. Different vocabulary, idioms, slang, and divergent interpretations of common words between domains can significantly degrade the quality of a SA. \citeA{ravi2015survey} provide an overview of work that addresses the challenge of cross-domain SA, but conclude that it is still an unsolved problem. Consequently, researchers should be careful when applying generic SA techniques to domain-specific corpora and vice versa. On data sets obtained from social media platforms, spam detection presents another issue that needs careful consideration. Many posts on social media are advertisements or were created by automated robots that post similar content multiple times. Not only can such duplicates ruin the quality of a collected data set, but they also dilute the content posted by real humans as spammers try to blend in as much as possible. Removing spam is viable through heuristics developed after manual inspection of a data set, for example by using word lists \shortcite{yao2020domain}. However, the precision of such methods must be scrutinized to not remove too much informative human-created content. Arguably the hardest challenge is coping with ambiguity and sarcasm. Using text as a medium of exchange of opinions can make these stylistic devices hard to identify even for humans. The sentence ``Yeah, company X is the best investment in the world...'' requires intonation or other cues to convey whether this is a sarcastic note or a serious opinion. This makes SA a problem on which even humans might not unanimously agree. If such uncertainty is present in a corpus of labeled data, it will also be present in any SA model that is developed based on this corpus.







% =======================================================================
\subsection{Research Gap}
\label{section-research-gap}
The literature on SA has devised plenty of theoretical contributions as well as functional model artifacts. However, almost none of these contributions apply to the domain of SA of financial social media posts. Many cover movie or product reviews, because labeled data is abundantly available in the form of star ratings that are attached to the textual review. Others comprise generic social media posts and address challenges like colloquial language and the use of emojis but are still not applicable to domain-specific texts \shortcite{mishev2020evaluation}. The few works that cover finance-related texts \cite{loughranMcD2011, araci2019finbert}  do not work on social media content because of its inherent challenges for automated SA. Table \ref{table-research-gap} provides an overview of the SA models that are predominantly used in the literature. The majority of them are dictionary-based unless otherwise noted ($\dagger$).

\input{assets/tables/static/research_gap_matrix.tex}

Most models, especially older ones developed before 2010, are neither finance-specific nor do they work on social media content. In recent years, the development of models that work on SM content has flourished and made major advancements, for example with the introduction of VADER \shortcite{hutto2014vader}, or, more recently, Twitter roBERTa \shortcite{barbieri2020tweeteval}. The literature on finance-specific texts is scarce. \shortciteA{loughranMcD2011} published a word list based on 10-K corporate filings, and more recently \shortciteA{araci2019finbert} published FinBERT which has been trained on news headlines. The gap in the existing research is consequently the intersection of financial social media posts. While \shortciteA{sohangir2018bigdata} experiment with deep learning models for predicting sentiment labels on StockTwits, they do not publish the final model artifact. This leaves NTUSD-Fin by \shortciteA{chen2018ntusd} who also developed a model for classifying StockTwits posts. Based on the labels that StockTwits users assign to their posts, they assemble a list of words with corresponding sentiment scores which is available upon request to the authors.

In order to add to the body of existing literature, this work aims to develop a sentiment model designed for working on financial social media posts. It will be guided by the following research questions.\newline
\textbf{\emph{RQ1:}} How can we design a functional model artifact that can extract an author's sentiment from financial social media posts?\newline
\textbf{\emph{RQ2:}} Does this model artifact outperform existing models from either the domain of financial texts or generic social media posts?\newline
\textbf{\emph{RQ3:}} Can a simple, domain-specific model outperform more generic LLMs?\newline
\textbf{\emph{RQ4:}} How does the performance of models trained on Twitter posts versus models trained on StockTwits posts compare?

\emph{RQ1} is aimed at the development and publication of a working model artifact. This is important to further the research in the field and enable other researchers and practitioners to run benchmarks on their own data sets to enhance the understanding of circumstances under which existing SA models do or do not work well. \emph{RQ2} studies the performance differences one can expect from using domain-specific models over generic models on domain-specific texts. This information will empower researchers to make better decisions regarding the SA model they use for their research. \emph{RQ3} tries to assess how LLMs, which are commonly used but expensive to train and deploy, compare to smaller models that have been trained on the target domain. It has been shown that for domain-specific computer vision applications, smaller, specialized models can outperform large off-the-shelf models for a fraction of the training- and deployment costs \shortcite{harl2022light}. This raises the question of whether the same holds for SA applications. Finally, \emph{RQ4} scrutinizes the differences between the domain of Twitter posts and StockTwits posts. This will help guide future research and might warrant the applications of models trained on StockTwits posts to other social media platforms and vice versa.


% =======================================================================
\subsection{Research Paradigm}

To answer the research questions posed in section \ref{section-research-gap} we adapt the process of Design Science Research as proposed by \shortciteA{kuechler2012dsrprocess} and \citeA{gregor2013dsr}. It consists of a series of five steps: Being aware of the problem, suggesting possible solutions, developing artifacts that instantiate these solutions, evaluating whether the developed artifacts solve the problem, and presenting a coherent conclusion that fosters future research \cite{kuechler2012dsrprocess}. Figure \ref{figure-dsr-process} displays the process steps along with the concrete application to answering the research questions presented in this work.

\input{assets/tikz_figures/dsr_process.tex}

The problem has been identified as a the lack of functional model artifacts that can be used for automated SA of financial social media posts. Consequently, we suggest developing such a model that can extract the opinion of an author regarding an investment from a social media post. Considering the lack of data in this domain, the development process will entail data collection, cleaning, and labeling. Subsequently, we can experiment with different ML and DL-based modeling approaches. All of these models will be evaluated and compared against existing models from the domain of finance or SM. Finally, the best models will be deployed as an installable python library, such that future research can conduct further benchmarking and improve upon the results.










