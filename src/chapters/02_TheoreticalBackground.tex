\section{Theoretical Background}



% =======================================================================
\subsection{Sentiment Analysis}  % TODO: wording
%todo: text here?
% -----------------------------------------------------------------------
\subsubsection{Operationalization of Sentiment}
% todo: vvv weak line of argument
Before surveying the literature on automated sentiment analysis models, the term ``Sentiment'' and its operationalization in the scientific literature need to be clearly defined. \shortciteA{munezero2014they} note that the terms \emph{emotion}, \emph{sentiment}, and \emph{opinion} are often used interchangeably, especially in the literature on NLP. However, there are important distinctions to be made between these terms. Emotions can be seen as moods that are not very long-lasting and are caused ``when we perceive positive or negative significant changes in our personal situation'' \shortcite[p.~3]{ben2001subtlety}. On the other hand, a sentiment is defined as ``an acquired and relatively permanent major neuropsychic disposition'' \cite[p.~16]{cattell1940sentiment}. This renders sentiments different from opinions, which are ``personal interpretations of information formed in the mind'' \cite[p.~4]{munezero2014they} and thus require a specific piece of information to interpret. %todo: wording
Despite the differences in the definition of these terms, they are used in ambiguous ways in the field of NLP. In particular, sentiment analysis and opinion mining refer to the same area of research \cite{liu2012book} and are more prevalent than studies on emotion extraction \shortcite{ravi2015survey}, \textcolor{red}{[ too long]} potentially because emotions are a more complex construct and cannot be fully conveyed through text \shortcite{munezero2014they}.

Another challenge is the operationalization of emotions, sentiments, or opinions. For example, emotion extraction studies like the ones conducted by \shortciteA{li2014text} or \shortciteA{aman2007identifying} tend to frame the problem as a six-class classification task where each class corresponds to one of the six basic emotions proposed by \shortciteA{ekman1971constants}. These six emotions are happiness, sadness, anger, surprise, disgust, and fear. However, \shortciteA{aman2007identifying} point out that the inter-rater reliability for this classification task on a corpus of text can be as low as 60\%, indicating that emotion extraction is a non-trivial task even for human annotators. There are variations of this measurement scale, like the ``Profile of Mood States'' questionnaire \shortcite{mcnair1971manual} which measures a different set of six emotional states. What all of these scales have in common is that they are multi-dimensional measurements to assess emotional states. \newline
On the other hand, the literature on sentiment analysis and opinion mining often employs simpler, even one-dimensional scales. The most prominent way of operationalizing sentiment is sentiment \emph{polarity}, which categorizes each unit of analysis as positive, negative, or, in some cases, neutral \cite{ravi2015survey}. Other approaches try to evaluate the sentiment in a piece of text as a real number between -1 and 1, a one to five-star rating (e.g. movie or product reviews), or another numeric score outside of any pre-determined interval.\newline
Besides the type of the employed sentiment measurement, the literature can also be categorized according to the unit of analysis it is concerned with. \shortciteA{liu2012book} distinguishes the units of analysis as being either document-level, sentence-level, or entity- and aspect-based. Document-level SA operates separately on every document in a corpus, e.g. each review for a product or movie or each social media post in a collection. This granularity is used most often as it aligns with the nature of a document as a self-contained piece of text by one author. For longer documents, sentence-level SA enables researchers to score sentiment on a per-sentence basis and subsequently calculate aggregate scores for the document. However, this comes at the expense of ignoring the inter-sentence context. Finally, aspect-based SA does not only generate sentiment scores but links these scores to the aspects they are referring to. This is mostly used in the analysis of product reviews, where some aspects of a product are rated positively while others are not, e.g. a computer with great battery life but low compute performance \shortcite{pontiki2016semeval}.

In addition to these definitions which are applicable in the field of NLP, other domains have their own definitions of sentiment. For the domain of finance, \citeA{aggarwal2019defining} provides an overview of all sentiment measures that have been historically used. In recent literature, two of them stick out as the most common: the Chicago Board Options Exchange Volatility Index (VIX) and the Put/Call Ratio (PCR). The calculation of the VIX is based on options on the Standard and Poor's 500 index (S\&P500) and represents the \emph{expected} level of volatility in the next month \shortcite{cboeVIX}. Its forward-looking nature makes it different from usual sentiment measurements which are based on historical data. Given that volatility is defined as the standard deviation of returns, it is not directly comparable to sentiment classes like ``positive'' or ``negative'' although lower volatility is generally associated with higher returns \shortcite{zare2013monetary}. The mathematically simpler Put/Call Ratio is the ratio between the volume of traded put options and call options. A PCR above 1 implies that put options are being traded more than call options indicating a negative market sentiment. Apart from these easily quantifiable versions of market sentiment in finance, the field starts recognizing that retail investors' emotions, sentiments, and opinions carry valuable information. The advent of behavioral finance describes and accounts for human biases in decision making processes, some of which can be assessed using sentiment analysis \shortcite{hirshleifer2015behavioral}.


% -----------------------------------------------------------------------
\subsubsection{Applications of Sentiment Analysis}

Sentiment analysis has a plethora of applications within academia as well as industry. Researchers have used social sentiment to study reactions to adverse events like hurricanes \shortcite{yao2020domain} or the COVID-19 pandemic \shortcite{dubey2020twitter}. Literature regarding political opinion mining demonstrates that sentiment extracted from posts on the microblogging platform Twitter correlates with public opinion to a point where it can be used to forecast election results \shortcite{o2010tweets, tumasjan2010predicting}.
Within finance, social sentiment obtained from microblogging platforms can help forecast stock market volatility \shortcite{antweiler2004all, audrino2020impact}, trading volume \shortcite{oliveira2017impact} and sometimes even future returns \shortcite{ren2018forecasting, wilksch2022predictive}.
In industry, sentiment analysis can be utilized to replace more costly surveys about consumer sentiment or mine information from product reviews. This presents businesses with the opportunity to assess their customers' needs and pain points in a faster, more efficient way and might -- in some cases -- alleviate the need for expensive focus groups. For example, this can be exploited by the film industry which can use social sentiment to forecast box office revenue and movie sales \shortcite{du2014box, rui2013whose}, or manufacturers who survey customers on what they like and dislike about a product (\textcolor{red}{cite aspect-based}).
In finance, service providers like brokerages are picking up this trend and start offering social sentiment scores as additional investment research to their customers \cite{ibkr-sentiment}. The common denominator between all these use cases is that they profit from more accurate SA models. Albeit automated SA models never perform with flawless accuracy, the closer they are to what humans consider to be ground truth, the better their downstream applications perform: descriptive statistics paint a more faithful picture of reality, forecasting models based on SA yield more accurate predictions (\textcolor{red}{cite!? W/A}), and for researchers, the quality of the obtained data more closely resembles the one of focus group or survey data.

\begin{itemize}[noitemsep]
	\item Other applications see Ravi \& Ravi and Liu Book (pdf page 19)
\end{itemize}

% =======================================================================
\subsection{Automated Sentiment Analysis}


% -----------------------------------------------------------------------
\subsubsection{Data Sets for Sentiment Analysis}

Every automated SA model -- whether dictionary-based or machine-learning-based -- requires a set of data to be built upon. For dictionaries, humans analyze large quantities of data to develop sets of words or phrases that carry sentiment and compile them in a machine-usable form. Machine learning models try to automate this process but still need a training data set that is often even larger than the ones used in manual dictionary creation. However, only a limited number of standardized, annotated data sets exist, as it is costly and labor-intensive to create such a data set. Therefore, many of the SA models in the literature have been trained or evaluated on similar data sets. Table \ref{most-used-datasets} provides an overview of the most frequently used data sets for conducting SA on texts from the domain of finance or social media.

\input{assets/tables/static/datasets_overview.tex}

 \shortciteA{malo2014good} compiled the \emph{Financial Phrasebank}, a data set of several thousand news headlines where each headline has been annotated as either positive, negative, or neutral by 16 independent annotators. For SA on social media posts, \shortciteA{rosenthal2017semeval} provide \emph{SemEval-2017 Task 4}, a large sample of tweets on current events, again labeled as one of three sentiment polarity classes. The largest corpus, \emph{Reuters TRC2}, released by \shortciteA{reuters-trc2}, contains 1.8 million Reuters news articles on various topics, including financial markets. However, the data set does not come with any kind of labels and is not exclusively geared towards performing SA. Consequently, it cannot be used for constructing dictionaries or training supervised machine learning models. Despite that, it can be used for pre-training large language models (LLM) which require vast unlabeled corpora for unsupervised training (see section \textcolor{red}{REF LLM sec}).\newline
 It is important to acknowledge the domain a training data set is sampled from as it significantly impacts the performance of the resulting classifier on its target data. For example, \shortciteA{al2020evaluating} show that researchers should always use models that can cope with slang, emojis, and typos when conducting SA on social media data. On the other hand, working with corporate financial filings requires a whole new approach and renders generic models ineffective \shortcite{loughranMcD2011}. Besides the vastly different vocabulary used in different domains, the complexity of a document can also require a change of the unit of analysis. Posts on the social network \emph{Reddit} tend to be much longer than Tweets, which are restricted to 280 characters in the first place. Thus, framing the SA of social media posts as a three-class classification problem might be adequate for Tweets (as a Tweet is likely to fall in one of the positive, negative, or neutral classes), but not so much for Reddit posts. Elaborate texts that span multiple paragraphs and might be a reply to a previous conversation can rarely be categorized as belonging to one of three classes. In these cases, sentence- or paragraph-based analysis is more suitable. \textcolor{blue}{TODO: actually pull Reddit posts and provide descriptive stats?}





Mention data-centric AI!
\begin{itemize}[noitemsep]
	\item describe different task types
	\begin{itemize}
		\item different scales
		\item different UoA: Sentence, Document, Aspect-based
	\end{itemize}
	\item describe different domains + challenges (financial posts on social media)
	\begin{itemize}[noitemsep]
		\item Platforms: Twitter, StockTwits, Reddit, Crypto-specific stuff
		\item Data characteristica: Reddit posts vs. Twitter posts (compare avg. length of each?)
\end{itemize}
	\item Use SemEval later on!
	\item We develop a data set that is a) finance-specific and b) sampled from a social media context. this is new. so far: financial, but not SM (FinancialPhrasebank) or vice versa (SemEval2017)
\end{itemize}

% -----------------------------------------------------------------------
\subsubsection{Dictionary-based Models}
\begin{itemize}
	\item see VADER paper: either words are pos/neg (LIWC, GI, HuLiu) OR scored with valence (ANEW, SentiWordNet)
	\item finish with benchmark paper from last year: VADER > everything
\end{itemize}

% -----------------------------------------------------------------------
\subsubsection{Machine-Learning-based Models}
\begin{itemize}[noitemsep]
	\item Note: there aren't really any non-DL ML models out there. Tho: there is a lot of research on ML models, buuut there are no published artifacts. These are mostly dictionaries/DL!
	\item Recurrent models
	\item Transformer models
	\item LLMs
\end{itemize}


\emph{Argument:} We can only consider models that are available as artifacts. Theoretical papers cannot be considered, as we can't reproduce their models/apply them in our experiments










% =======================================================================
\subsection{Research Gap}
\begin{itemize}[noitemsep]
	\item performance benchmarks of LLMs (and all other models too, right?) on domain-specific texts
	\item development of a data set: There is also no data set on SM financial data 	
\end{itemize}

\input{assets/tables/static/research_gap_matrix.tex}





% =======================================================================
\subsection{Conceptual Framework}

In this work, \dots 

\begin{itemize}[noitemsep]
	\item In this work: focus on SA and opinion mining (i.e. everything on uni-dimensional scales!) as defined in the field of NLP. This ``Social Sentiment'' definition is also slowly getting traction in finance, btw. Also here in this work: document-level!
\end{itemize}



\input{assets/tikz_figures/conceptual_framework.tex}

\emph{Research Questions}: Do LLMs perform well on domain-specific sentiment ananylsis tasks? How do they compare to smaller, machine-learning-based models as well as generic off-the-shelf models? Can simple, domain-specific models outperform LLMs/fine-tuned LLMs? Even if not, how do their memory/compute/cost footprints compare? 

Models to build:

\input{assets/tables/static/models_to_build.tex}








