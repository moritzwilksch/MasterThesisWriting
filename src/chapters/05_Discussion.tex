\section{Discussion}

This work set out to answer four research questions about the design of a functional market sentiment model (RQ1), its performance compared to models from adjacent domains (RQ2), as well as the relationship between model size and performance (RQ3), and discrepancies between data sources used for training and evaluation (RQ4).

The experimental results establish a series of findings that are relevant for designing a functional model artifact for market sentiment analysis (RQ1). First, the results indicate that while compiling the dataset used for model training or evaluation, data filtering is an essential step to uphold data quality. Even after running an API query that has been designed to exclude tweets that are unusable for the analysis, our conservative data filtering mechanisms remove more than a quarter of the data collected. Nevertheless, the dataset still contains spam posts that had to be labeled as neutral. This confirms previous findings which suggest that spam detection is among the top challenges for sentiment analysis of online content \shortcite{hussein2018survey}. Second, our results show that utilizing human annotations for domain-specific tasks can amortize quickly: The learning curves demonstrate that we achieve acceptable performance with relatively few data points. Using naturally occurring labels, on the other hand, is not possible for finance-related tweets because such labels do not exist. On platforms like StockTwits, where natural labels exist in the form of author-assigned sentiment tags, they have been shown to be sparse and unreliable \shortcite{chen2020finsome}. Our findings contradict suggestions by \shortciteA{renault2020sentiment} who shows that a classifier's performance can increase significantly if trained on dataset sizes up to 250,000 messages. However, their results are based on the two-class, author-assigned labels of StockTwits posts which need not be comparable to the labeled data in this work. Nonetheless, our results can confirm the suggestions regarding text preprocessing that \shortciteA{renault2020sentiment} makes. We also find emojis to be indicative of sentiment thus warranting their inclusion in the cleaned texts. Furthermore, the hyperparameter selection demonstrates that the use of n-grams significantly improves the ML models' predictive performance. Additional text preprocessing like stemming or stop word removal harmed predictive performance according to exploratory experiments. Finally, comparing the proposed models against each other suggests that for small datasets, using simple models is an advantageous strategy to combat overfitting. Despite strong regularization, the neural networks do not manage to outperform the logistic regression or SVM on our out-of-sample benchmarks.

The evaluation of all models on the collected dataset of tweets (RQ2) shows that all proposed models outperform the existing ones. The dictionary-based models perform notably worse than other existing models. In generic sentiment analysis, VADER performs best of its class achieving accuracies of up to 72\% \shortcite{al2020evaluating}. Our results show that it hardly outperforms a random guessing strategy when applied to the market sentiment analysis task. Even NTUSD-Fin only performs slightly better, despite being developed on finance-related social media posts. The performance gap between VADER and NTUSD-Fin is larger on SemEval than on any other dataset, an observation that our experimental setup cannot explain.
 This behavior suggests that the rigid structure of dictionary-based models diminishes their performance. The performance of both TwitterRoBERTa and FinBERT deliver further evidence for this claim. TwitterRoBERTa was trained on generic Twitter messages while FinBERT was trained on a set of news headlines. Nevertheless, they perform significantly better than their dictionary-based counterparts as the sub-word tokenization they employ allows them to process out-of-vocabulary tokens. Surprisingly, TwitterRoBERTa performs on par with FinBERT, although the majority of its training corpus is not related to finance or business. However, even these two neural-network-based LLM are inferior to all of our proposed models. This holds on the dataset of tweets as well as the SemEval and Fin-SoMe dataset consisting of StockTwits messages.\newline
 Furthermore, the results suggest that extracting market sentiment from tweets is a harder task than sentiment analysis of news headlines. BERT-based models can classify news headlines with accuracies of up to 86\% (F1 score: 0.84) \shortcite{araci2019finbert} while our best model achieves an accuracy of around 64\% (F1 score: 0.64). A potential reason for this is that tweets contain more ambiguous messages, sarcasm, or slang words than editorial content like news headlines. Similarly, TwitterRoBERTa by \shortciteA{barbieri2020tweeteval} achieved a macro-averaged recall of 0.729 while our proposed model only scores 0.611 providing further evidence for a performance gap between generic and domain-specific sentiment analysis.

Our results also provide evidence for the claim that small domain-specific models can outperform LLMs for classifying market sentiment (RQ3). The large performance gap between the complex, pre-trained models and a logistic regression classifier can be explained by the domain-specific vocabulary the custom models have learned. Many tokens that indicate a \emph{market} sentiment do not carry any sentiment in general English language making it hard for generic pre-trained models to correctly classify documents using them.  For the ML models, the pre-processing steps and tokenizer hyperparameters create a feature space that allows them to make accurate predictions based on sentiment-laden terms. Similarly, the DL models we trained from scratch were able to learn semantically rich word embeddings where words that indicate similar market sentiments are assigned similar embeddings. Furthermore, the results guide researchers in training custom domain-specific models. We show that using pre-trained word embeddings -- a common practice in NLP -- can be detrimental to classifier performance. While they are essential for achieving state-of-the-art performance on tasks like neural machine translation \shortcite{qi2018and}, they should not be used for sentiment analysis out of the box. This issue has previously been identified by research which suggests adapting the vectors to domain-specific tasks like sentiment analysis \shortcite{rezaeinia2017improving} or use in medical coding \shortcite{patel2017adapting}.\newline
However, in settings with larger corpora of labeled data, neural networks might achieve higher performance than the ML models trained in this work. The chosen hyperparameters indicate that overfitting is a severe issue of neural networks in such small data settings, hence large dropout probabilities were selected to facilitate the models' ability to generalize. Nonetheless, the relatively large performance decay of the fine-tuned BERT model, the transformer neural network, and the recurrent model on external data suggest they are most affected by overfitting issues. On larger corpora, however, they might be able to alleviate the logistic regression's issues with detecting implicit negation.
The comparison of all models on multiple datasets shows that an SVM and a logistic regression model can outperform multiple different neural-network-based architectures. This is especially relevant when considering the computational requirements for training and inference. Using the proposed logistic regression model over any of the BERT-based LLMs incurs 1000 times less compute cost during inference alone. Compared to the SVM, the model is still 50 times faster. For other tasks, previous research shows that small domain-specific models can occasionally outperform larger generic ones. For example, \shortciteA{harl2022light} designed a custom CNN for solar wafer defect detection and manage to deliver the same performance as larger image models at a fraction of the cost. This work can inspire future research to not only compare models based on their performance characteristics but also on their compute footprints to encourage more resourceful machine learning which considers the return on investment for model training and deployment.

Finally, our modeling results and the descriptive analysis of the Twitter and StockTwits datasets allow us to make an informed recommendation regarding whether the two data sources can be used interchangeably (RQ4). The descriptive analysis reveals that the characteristics of texts posted on both platforms differ in length, content, and class distribution, even after filtering spam posts from the Twitter data. A portion of these differences can be attributed to the different sampling timeframes. However, given that the platform lacks a ``neutral'' label, only 17\% of users on StockTwits use the labeling feature \shortcite{li2017learning}, and some of these labels are wrong \shortcite{chen2020finsome}, the quality of data obtained from StockTwits remains questionable. When applying the proposed models to the StockTwits dataset, we observed a large decay in predictive performance supporting this claim. Moreover, this performance decrease happened even though \shortciteA{chen2020finsome} utilized multiple human annotators to compile a gold standard dataset. Thus, our findings indicate that researchers should not use data from StockTwits to train models that will be applied to tweets, contradicting previous suggestions made by \shortciteA{renault2020sentiment}.

The presented work has implications for future theoretical and practical contributions. From a theoretical perspective, our results can be utilized to design future studies on building sentiment classifiers. The insights regarding the text representation, modeling, and the effect of dataset size on the classifier's performance can be used to rule out unpromising configurations.
 As for practical contributions, we show that researchers need to consider the operationalization of sentiment when they use it as an explanatory variable. This entails intently choosing a sentiment scale and model. To make the right choice, we suggest that researchers manually label a small subset of their data, for example, 1,000 documents. Based on this subset, they can evaluate which model best suits their needs. For market sentiment analysis, we have shown that researchers should prefer our proposed model for data from Twitter and either the proposed model, TwitterRoBERTa, or FinBERT for analyzing StockTwits posts. The otherwise popular VADER model is not suited for this task, as it classifies too many documents as neutral because its dictionary-based structure cannot cope with the domain-specific vocabulary.\newline
Overall, this work improves upon the state-of-the-art in market sentiment analysis of finance-related social media posts. The lack of publicly available models and datasets tailored to this purpose has forced previous research to utilize generic sentiment models from adjacent domains which do not provide accurate sentiment assessments. This is problematic as previous research suggests that more accurate sentiment analysis can improve the performance of downstream prediction models \shortcite{wilksch2022predictive}. The artifact we designed and published using the Design Science Research methodology helps alleviate these issues and can be leveraged by future research, for example for market volatility or return prediction.\newline
However, this work also raised questions that can be addressed in future research. Among those is the issue of handling neutral, uncertain, or tweets without sentiment. Especially for uncertain documents, aspect-based sentiment analysis could be utilized to dissect which parts of a text indicate positive or negative sentiment for different entities instead of aggregating them into an ``uncertain'' category. We have made the point that framing market sentiment analysis as a two-class problem (bullish vs. bearish) is too simplistic due to the large amounts of documents that do not carry a clear market sentiment. Similarly, using a third ``neutral'' category might oversimplify the nuances between the absence of sentiment, uncertainty, or spam.
Furthermore, future research can explore the intricacies of finance-related tweets by compiling a larger gold-standard dataset. Historically, researchers have been obtaining data from StockTwits as some of it is naturally labeled. Our results indicate that tweets and StockTwits posts are not necessarily comparable, therefore warranting future examination of data obtained from Twitter. Assembling a larger corpus of manually annotated tweets would also allow researchers to shed light on when neural-network-based solutions can outperform simple classification models -- a question that we could not answer given the generalization problems of complex models trained on small datasets.

The findings we present are subject to several limitations. The entire project is based on English texts collected from Twitter by querying for cashtags of large US companies. Therefore, the findings need not generalize to other markets like the cryptocurrency market or equity markets in Asia. Certainly, the proposed model does not generalize to texts that are not English, hence a language filter should be used before applying the model to a corpus of text. We already show that our models' performance degrades when applied to content from StockTwits, therefore researchers should apply the model to other data sources (like Reddit or Facebook) with care. Furthermore, the model was trained on a small dataset from a fixed period. A larger dataset or sampling of a longer timeframe of tweets could improve the model's capabilities. It might also affect our conclusions regarding the neural networks' tendency to overfit as this issue is expected to recede with a larger training corpus. Further research is necessary as to how model performance decays over time as the data distribution between current online discussions and the training data drift apart.

