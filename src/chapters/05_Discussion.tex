\section{Discussion}

This work set out to answer four research questions about the design of a functional market sentiment model (RQ1), its performance compared models from adjacent domains (RQ2), as well as the relationship between model size and performance (RQ3) and discrepancies between data sources (RQ4).

The experimental results establish a series of findings that are relevant for designing a functional model artifact for market sentiment analysis (RQ1). First, the results indicate that while compiling the dataset used for model training or evaluation, data filtering is an essential step to uphold data quality. Even after running an API query that has been designed to exclude tweets that are unusable for the analysis, our conservative data filtering mechanisms remove more than a quarter of the data collected. Nevertheless, the dataset still contains spam posts that had to be labeled as neutral. This confirms previous findings which suggest that spam detection is among the top challenges for sentiment analysis of online content \shortcite{hussein2018survey}. Second, our results show that utilizing human annotations for domain-specific tasks can amortize quickly: The learning curves demonstrate that we achieve acceptable performance with relatively few data points. Using naturally occurring labels, on the other hand, is not possible for finance-related tweets because such labels do not exist. On platforms like StockTwits, where natural labels exist in the form of author-assigned sentiment tags, they have been shown to be sparse and unreliable \shortcite{chen2020finsome}. Our findings contradict suggestions by \shortciteA{renault2020sentiment} who shows that a classifier's performance can increase significantly if trained on dataset sizes up to 250,000 messages. However, their results are based on the two-class, author-assigned labels of StockTwits posts which need not be comparable to the labeled data in this work. Nonetheless, our results can confirm the suggestions regarding text preprocessing that \shortciteA{renault2020sentiment} makes. We also find emojis to be indicative of sentiment thus warranting their inclusion in the cleaned texts. Furthermore, the hyperparameter selection demonstrates that the use of n-grams significantly improves the ML models' predictive performance. Further text preprocessing like stemming or stop word removal had a negative effect on predictive performance according to exploratory experiments. Finally, comparing the proposed models against each other suggests that for small datasets, using simple models is an advantageous strategy to combat overfitting. Despite strong regularization, the neural networks do not manage to outperform the logistic regression or SVM on our out-of-sample benchmarks.

The evaluation of all models on the collected dataset of tweets (RQ2) show that all proposed models outperform the existing ones. The dictionary-based models perform notably worse than other existing models. In generic sentiment analysis, VADER performs best of its class achieving accuracies of up to 72\% \shortcite{al2020evaluating}. Our results show that it hardly outperforms a random guessing strategy when applied to the market sentiment analysis task. Even NTUSD-Fin only performs slightly better, despite being developed on finance-related social media posts. The performance gap between VADER and NTUSD-Fin is larger on SemEval than on any other dataset, an observation which our experimental setup cannot explain.
 This behavior suggests that the rigid structure of dictionary-based models diminishes their performance. The performance of both TwitterRoBERTa and FinBERT deliver further evidence for this claim. TwitterRoBERTa was trained on generic Twitter messages while FinBERT was trained on a set of news headlines. Nevertheless, they perform significantly better than their dictionary-based counterparts as the sub-word tokenization they employ allows them to process out-of-vocabulary tokens. Surprisingly, TwitterRoBERTa performs on par with FinBERT, although its training corpus is not related to finance or business. However, even these two LLM-based neural networks are inferior to all of our proposed models. This holds true on the dataset of tweets as well as the Fin-SoMe dataset consisting of StockTwits messages.


Our results also provide evidence for the claim that small domain-specific models can outperform LLMs for classifying market sentiment (RQ3). The large performance gap between the complex, pre-trained models and a logistic regression classifier can be explained by the domain-specific vocabulary the custom models have learned. Many tokens that clearly indicate a \emph{market} sentiment do not carry any sentiment in general English language making it hard for generic pre-trained models to correctly classify documents using them.  For the ML models, the pre-processing steps and tokenizer hyperparameters create a feature space that allows them to make accurate predictions based on sentiment-laden terms. Similarly, the DL models we trained from scratch were able to learn semantically rich word embeddings where words which indicate similar market sentiments are assigned similar embeddings. Furthermore, the results provide guidance for researchers training custom domain-specific models. We show that using pre-trained word embeddings -- a common practice in NLP -- can be detrimental to classifier performance. While they are essential for achieving state of the art performance on tasks like neural machine translation \shortcite{qi2018and}, they should not be used for sentiment analysis out of the box. This issue has previously been identified by research which suggests adapting the vectors to domain-specific tasks like sentiment analysis \shortcite{rezaeinia2017improving} or use in medical coding \shortcite{patel2017adapting}.\newline
However, in settings with larger corpora of labeled data, neural networks might achieve higher performance than the ML models trained in this work. The chosen hyperparameters indicate that overfitting is a severe issue of neural networks in such small data settings, hence large dropout probabilities were selected to facilitate the models' ability to generalize. Nonetheless, the relatively large performance decay of the fine-tuned BERT model, the transformer neural network and the recurrent model on external data suggest they they are most affected by overfitting issues.
The comparison of all models on multiple dataset shows that an SVM and a logistic regression model can outperform multiple different neural-network-based architectures. This is especially relevant when considering the computational requirements for training and inference. Using the proposed logistic regression model over any of the BERT-based LLMs incurs 1000 times less compute cost during inference alone. Compared to the SVM, the model is still 50 times faster. For other tasks, previous research shows that small domain-specific models can occasionally outperform larger generic ones. For example, \shortciteA{harl2022light} designed a custom CNN for solar wafer defect detection and manage to deliver the same performance as larger image models at a fraction of the cost. This work can inspire future research to not only compare models based on their performance characteristics but also on their compute footprints to encourage more resourceful machine learning which considers the return on investment for model training and deployment.

Finally, our modeling results and the descriptive analysis of the Twitter and StockTwits datasets allow us to make an informed recommendation regarding whether the two data sources can be used interchangeably (RQ4). The descriptive analysis reveals that the characteristics of texts posted on both platforms differ in length and content. A portion of these differences can be attributed to the different sampling timeframes. However, given that the platform lacks a ``neutral'' label, only 17\% of users on StockTwits use the labeling feature \shortcite{li2017learning}, and some of these labels are wrong \shortcite{chen2020finsome}, the quality of data obtained from StockTwits remains questionable. When applying the proposed models to the StockTwits dataset, we observed a large decay in predictive performance supporting this claim. Moreover, this performance decrease was despite the fact that \shortciteA{chen2020finsome} utilized multiple human annotators to compile this gold standard dataset. Thus, our findings indicate that researchers should not use data from StockTwits to train models that will be applied to tweets, contradicting previous suggestions made by \shortciteA{renault2020sentiment}.


Overall, this work improves upon the state of the art in market sentiment analysis of finance-related social media posts. The lack of publicly available models and datasets tailored to this purpose has forced previous research to utilize generic sentiment models from adjacent domains which do not provide accurate sentiment assessments. This is problematic as previous research suggests that more accurate sentiment analysis can improve the performance of downstream prediction models \shortcite{wilksch2022predictive}. The artifact we designed and published using the Design Science Research methodology helps alleviate these issues and can be leveraged by future research, for example for market volatility or return prediction.\newline
However, this work also raised questions that can be addressed in future research. Among those is the issue of handling neutral, uncertain, or tweets without sentiment. Especially for uncertain documents, aspect-based sentiment analysis could be utilized to dissect which parts of a text indicate positive or negative sentiment with respect to different entities instead of aggregating them into an ``uncertain'' category. We have made the point that framing market sentiment analysis as a two-class problem (bullish vs. bearish) is too simplistic due to the large amounts of documents that do not carry a clear market sentiment. Similarly, using a third ``neutral'' category might oversimplify the nuances between absence of sentiment, uncertainty, or spam.
Furthermore, future research can explore the intricacies of finance-related tweets by compiling a larger gold-standard dataset. Historically, researchers like obtaining data from StockTwits as some if it is naturally labeled. Our results indicate that tweets and StockTwits posts are not necessarily comparable, therefore warranting future examination of data obtained from Twitter. Assembling a larger corpus of manually annotated tweets would also allow researchers to shed light on when neural-network-based solutions can outperform simple classification models -- a question that we could not answer given the generalization problems of complex models trained on small datasets.

== Paragraph 5==\\
Implications, theoretical + practical

== Paragraph 6 ==\\
Limitations + future research










\newpage
Discussion points
\begin{itemize}[noitemsep]
	\item re: dataset differences: espc. intereresting given that we filtered out loads of spam (i.e. neutral posts), so w/o filtering there would be many more neutrals, not more positive posts?
	\item differentiating between uncertain and no sentiment is a hard task. performance does not warrant building a 4-class model, so just stick to three classes. It was also hardest to label!
	\item ``Recommend SA model for different use cases'' (see DSR flowchart) -> when finbert, twitterroBERTa, ...?
	\item Future research: aspect-based SA to remedy the uncertain category
\end{itemize}